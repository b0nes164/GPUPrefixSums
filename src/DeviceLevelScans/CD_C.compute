/******************************************************************************************************************
 * Inclusive Chained Decoupled Lookback Scan Implementation
 * Author:  Thomas Smith 3/9/2023
 *
 * Variant: Warp-sized radix KoggeStone scans embedded into BrentKungBlelloch
 *          Global Memory
 *          Fixed Partition Size
 *          Variable ThreadBlocks/WorkGroups
 *          Fixed Partitions
 *
 * License: The Unlicense
 *          This is free and unencumbered software released into the public domain.
 *          For more information, please refer to the repository license or <https://unlicense.org>
 *   
 * Based off of Research by:
 *          Duane Merrill, Nvidia Corporation
 *          Michael Garland, Nvidia Corporation
 *          https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back
 *
 * This implementation does not constitute ANY form of endorsement or promotion
 * from its authors or Nvidia Corporation. In no event shall Nvidia Corporation
 * be held liable for ANY damages arising in any way out of the use of this 
 * software. The implementation author is not affiliated in ANY way with Nvidia
 * Corporation.
 *
 ******************************************************************************/
#pragma use_dxc
#pragma kernel Init
#pragma kernel InitPartitionDescriptors
#pragma kernel ChainedDecoupledScanC

#define GROUP_SIZE  1024

#define LANE_COUNT  32  // <--------------------------- Nvidia lanecount; change depending on hardware
#define LANE_MASK   31
#define LANE_LOG    5
#define WARPS_PER_BLOCK 32
//#define LANE_COUNT    64 <--------------------------- AMD
//#define LANE_MASK     63
//#define LANE_LOG      6
//#define WAPRS_PER_BLOCK  16

extern uint e_size;
extern uint e_partitions;

#define FLAG_NOT_READY  0
#define FLAG_AGGREGATE  1
#define FLAG_PREFIX     2
#define FLAG_MASK       3

globallycoherent RWBuffer<uint> b_state;
RWBuffer<uint> b_prefixSum;

groupshared bool g_groupFlags[WARPS_PER_BLOCK];
groupshared uint g_agrMem[LANE_COUNT];

[numthreads(GROUP_SIZE, 1, 1)]
void Init(int3 id : SV_DispatchThreadID)
{
    for (int i = id.x; i < e_size; i += GROUP_SIZE * 32)
        b_prefixSum[i] = 1;
}

[numthreads(GROUP_SIZE, 1, 1)]
void InitPartitionDescriptors(int3 id : SV_DispatchThreadID)
{
    //Initialize the status flags
    if (id.x < e_partitions + 1)
        b_state[id.x] = FLAG_NOT_READY;
}

[numthreads(GROUP_SIZE, 1, 1)]
void ChainedDecoupledScanC(int3 gtid : SV_GroupThreadID)
{
    //Atomically increment counter 
    int partitionIndex = 0;
    int threadBlocks = ceil((e_size * 1.0f) / GROUP_SIZE);
    if(gtid.x == 0)
    {
        InterlockedAdd(b_state[threadBlocks], 1, partitionIndex);
        for (int k = 0; k < WARPS_PER_BLOCK; k++)
            g_agrMem[k] = partitionIndex;
    }
    if (gtid.x < WARPS_PER_BLOCK)
        g_groupFlags[gtid.x] = true;
    AllMemoryBarrierWithGroupSync();
    partitionIndex = WaveReadLaneFirst(g_agrMem[gtid.x >> LANE_LOG]);
    
    int partitionSize = GROUP_SIZE;
    int partitionStart = partitionIndex * partitionSize;
    int partitionEnd = partitionIndex == threadBlocks - 1 ? e_size : (partitionIndex + 1) * partitionSize;
    partitionSize = partitionEnd - partitionStart;
    
    //aggregate
    if(gtid.x < partitionSize)
    {
        const int t = gtid.x + partitionStart;
        b_prefixSum[t] += WavePrefixSum(b_prefixSum[t]);
    }
    AllMemoryBarrierWithGroupSync();
    
    if (gtid.x < (partitionSize >> LANE_LOG))
    {
        const int t = ((gtid.x + 1) << LANE_LOG) + partitionStart - 1;
        b_prefixSum[t] += WavePrefixSum(b_prefixSum[t]);
    }
    AllMemoryBarrierWithGroupSync();
    
    //Set flag payload
    if (gtid.x == 0)
    {
        if(partitionIndex == 0)
            InterlockedOr(b_state[partitionIndex], FLAG_PREFIX ^ (b_prefixSum[partitionEnd - 1] << 2));
        else
            InterlockedOr(b_state[partitionIndex], FLAG_AGGREGATE ^ (b_prefixSum[partitionEnd - 1] << 2));
    }
    
    //lookback
    int aggregate = 0;
    if(partitionIndex != 0)
    {
        int indexOffset = 0;
        do
        {
            if (gtid.x < LANE_COUNT)
            {       
                //"Walk" down the partition aggregates
                for (int i = partitionIndex - (gtid.x + indexOffset + 1); 0 <= i; i -= LANE_COUNT)
                {
                    uint flagPayload = b_state[i];
                    int prefixIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_PREFIX ? LANE_COUNT : 0));
                    int gapIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_NOT_READY ? LANE_COUNT : 0));
                    if (prefixIndex < gapIndex)
                    {
                        aggregate += WaveActiveSum(gtid.x <= prefixIndex ? (flagPayload >> 2) : 0);
                        if (gtid.x == 0)
                        {
                            InterlockedExchange(b_state[partitionIndex], FLAG_PREFIX ^ ((aggregate + b_prefixSum[partitionEnd - 1]) << 2), flagPayload);
                            for (int k = 0; k < WARPS_PER_BLOCK; k++)
                            {
                                g_groupFlags[k] = false;
                                g_agrMem[k] = aggregate;
                            }
                        }
                        break;
                    }
                    else
                    {
                        aggregate += WaveActiveSum(gtid.x < gapIndex ? (flagPayload >> 2) : 0);
                        indexOffset += gapIndex;
                        break;
                    }
                }
            }
            AllMemoryBarrierWithGroupSync();
        } while (WaveReadLaneFirst(g_groupFlags[gtid.x >> LANE_LOG]));
    }
    
    //Fan aggregates
    aggregate = WaveReadLaneFirst(g_agrMem[gtid.x >> LANE_LOG]);
    int t = (gtid.x + 1 << LANE_LOG) - 1;
    if (t < partitionSize)
        b_prefixSum[t + partitionStart] += aggregate;
    AllMemoryBarrierWithGroupSync();
    
    const int lane = gtid.x & LANE_MASK;
    if(gtid.x < partitionSize)
    {
        if (gtid.x > LANE_MASK)
        {
            if (lane < LANE_MASK)
                b_prefixSum[gtid.x + partitionStart] += b_prefixSum[((gtid.x >> LANE_LOG) << LANE_LOG) + partitionStart - 1];
        }
        else if (lane < LANE_MASK)
            b_prefixSum[gtid.x + partitionStart] += aggregate;
    }
}