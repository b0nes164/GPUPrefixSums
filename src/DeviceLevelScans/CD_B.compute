/******************************************************************************************************************
 * Inclusive Chained Decoupled Lookback Scan Implementation
 * Author:  Thomas Smith 3/8/2023
 *
 * Variant: Raking warp-sized radix reduce then scan
 *          Shared Memory
 *          Fixed Partition Size
 *          Fixed ThreadBlocks/WorkGroups
 *          Variable Partitions
 *          No Atomic Loads
 *
 * License: The Unlicense
 *          This is free and unencumbered software released into the public domain.
 *          For more information, please refer to the repository license or <https://unlicense.org>
 *   
 * Based off of Research by:
 *          Duane Merrill, Nvidia Corporation
 *          Michael Garland, Nvidia Corporation
 *          https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back
 *
 * This implementation does not constitute ANY form of endorsement or promotion
 * from its authors or Nvidia Corporation. In no event shall Nvidia Corporation
 * be held liable for ANY damages arising in any way out of the use of this 
 * software. The implementation author is not affiliated in ANY way with Nvidia
 * Corporation.
 *
 ******************************************************************************/
#pragma use_dxc
#pragma kernel Init
#pragma kernel InitPartitionDescriptors
#pragma kernel ChainedDecoupledScanB

#define PARTITION_SIZE 1024
#define GROUP_SIZE 1024 //note this is purely for the init kernels

#define LANE_COUNT 32 //<--------------------------- For Nvidia; change depending on hardware
#define LANE_MASK 31
#define LANE_LOG 5


//#define LANE_COUNT 64 <--------------------------- For AMD 
//#define LANE_MASK 63
//#define LANE_LOG 6

extern uint e_size;
extern uint e_partitions;

#define FLAG_NOT_READY  0
#define FLAG_AGGREGATE  1
#define FLAG_PREFIX     2

globallycoherent RWBuffer<uint> b_state;
RWBuffer<uint> b_prefixSum;
groupshared uint g_sharedMem[PARTITION_SIZE];

[numthreads(GROUP_SIZE, 1, 1)]
void Init(int3 id : SV_DispatchThreadID)
{
    for (int i = id.x; i < e_size; i += GROUP_SIZE * 32)
        b_prefixSum[i] = 1;
}

[numthreads(GROUP_SIZE, 1, 1)]
void InitPartitionDescriptors(int3 id : SV_DispatchThreadID)
{
    //Initialize the status flags
    int threadBlocks = ceil((e_partitions * 3 + 1) * 1.0f / GROUP_SIZE);
    for (int i = id.x; i < e_partitions * 3 + 1; i += GROUP_SIZE * threadBlocks)
        b_state[i] = FLAG_NOT_READY;
}

[numthreads(LANE_COUNT, 1, 1)]
void ChainedDecoupledScanB(int3 gtid : SV_GroupThreadID)
{
    int partitions = e_partitions;
    int partitionIndex, partitionSize, partitionStart;
    int subPartSize, subPartStart, subPartEnd;
    
    //Atomically increment partition index to "guarantee" forward progress
    if (gtid.x == 0)
    {
        InterlockedAdd(b_state[partitions * 3], 1, partitionIndex);
        g_sharedMem[0] = partitionIndex;
    }
    partitionIndex = WaveReadLaneFirst(g_sharedMem[0]);
    
    while (partitionIndex < partitions)
    {
        partitionStart = partitionIndex * PARTITION_SIZE;
        partitionSize = partitionIndex == partitions - 1 ? e_size - partitionStart : PARTITION_SIZE;
        
        //Load into shared memory
        for (int j = gtid.x; j < partitionSize; j += LANE_COUNT)
            g_sharedMem[j] = b_prefixSum[j + partitionStart];
        
        subPartSize = partitionSize >> LANE_LOG;
        subPartStart = subPartSize * gtid.x;
        subPartEnd = gtid.x == LANE_MASK ? partitionSize - 1 : subPartSize * (gtid.x + 1) - 1;
        
        //Per thread serial prefix sum
        for (j = subPartStart + 1; j <= subPartEnd; ++j)
            g_sharedMem[j] += g_sharedMem[j - 1];
        
        //KoggeStone
        g_sharedMem[subPartEnd] += WavePrefixSum(g_sharedMem[subPartEnd]);
        
        if (gtid.x == 0)
        {
            if (partitionIndex == 0)
            {
                InterlockedOr(b_state[(partitions << 1) + partitionIndex], g_sharedMem[partitionSize - 1]);
                InterlockedOr(b_state[partitionIndex], FLAG_PREFIX);
            }
            else
            {
                InterlockedOr(b_state[partitions + partitionIndex], g_sharedMem[partitionSize - 1]);
                InterlockedOr(b_state[partitionIndex], FLAG_AGGREGATE);
            }
        }

        //lookback
        int aggregate = 0;
        bool breaker = true;
        if (partitionIndex != 0)
        {
            int indexOffset = 0;
            do
            {
                for (int i = partitionIndex - (gtid.x + indexOffset + 1); 0 <= i; i -= LANE_COUNT)
                {
                    int t = b_state[i];
                    int prefixIndex = WaveActiveMin(gtid.x + LANE_COUNT - (t == FLAG_PREFIX ? LANE_COUNT : 0));
                    int gapIndex = WaveActiveMin(gtid.x + LANE_COUNT - (t == FLAG_NOT_READY ? LANE_COUNT : 0));
                    if (prefixIndex < gapIndex)
                    {
                        breaker = false;
                        aggregate += WaveActiveSum(gtid.x <= prefixIndex ?
                                (gtid.x == prefixIndex ? b_state[(partitions << 1) + i] : b_state[partitions + i]) : 0);
                        if (gtid.x == 0)
                        {
                            InterlockedAdd(b_state[(partitions << 1) + partitionIndex], aggregate + g_sharedMem[partitionSize - 1]);
                            InterlockedAdd(b_state[partitionIndex], 1);
                        }
                        break;
                    }
                    else
                    {
                        aggregate += WaveActiveSum(gtid.x < gapIndex ? b_state[partitions + i] : 0);
                        indexOffset += gapIndex;
                        break;
                    }
                }
            } while (WaveReadLaneFirst(breaker));
        }
        
        //propagate
        aggregate = WaveReadLaneFirst(aggregate);
        if(gtid.x > 0)
        {
            for (j = subPartStart; j < subPartEnd; ++j)
                g_sharedMem[j] += g_sharedMem[subPartStart - 1] + aggregate;
            g_sharedMem[j] += aggregate;
        }
        
        //fill in the subpartition
        for (j = gtid.x; j < subPartSize; j += LANE_COUNT)
            g_sharedMem[j] += aggregate;
        
        //push back to global memory
        for (j = gtid.x; j < partitionSize; j += LANE_COUNT)
            b_prefixSum[j + partitionStart] = g_sharedMem[j];
        
        if (gtid.x == 0)
        {
            InterlockedAdd(b_state[partitions * 3], 1, partitionIndex);
            g_sharedMem[0] = partitionIndex;
        }
        partitionIndex = WaveReadLaneFirst(g_sharedMem[0]);
    }
}