/*******************************************************************************
 * Inclusive Chained Decoupled Lookback Scan Implementation
 * Author:  Thomas Smith 3/13/2023
 *
 * Variant: Warp-sized radix KoggeStone scans embedded into BrentKungBlelloch
 *          Global Memory
 *          Variable Partition Size
 *          Fixed ThreadBlocks/WorkGroups
 *          Fixed Partitions
 *          Atomic Loads
 *          Flag Packing
 *
 *
 * License: The Unlicense
 *          This is free and unencumbered software released into the public domain.
 *          For more information, please refer to the repository license or <https://unlicense.org>
 *   
 * Based off of Research by:
 *          Duane Merrill, Nvidia Corporation
 *          Michael Garland, Nvidia Corporation
 *          https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back
 *
 * This implementation does not constitute ANY form of endorsement or promotion
 * from its authors or Nvidia Corporation. In no event shall Nvidia Corporation
 * be held liable for ANY damages arising in any way out of the use of this 
 * software. The implementation author is not affiliated in ANY way with Nvidia
 * Corporation.
 *
 ******************************************************************************/
#pragma use_dxc
#pragma require Int64

#pragma kernel Init
#pragma kernel InitPartitionDescriptors
#pragma kernel ChainedDecoupledScanAtomic

#define GROUP_SIZE          1024
#define THREAD_BLOCKS       256
#define THREAD_BLOCKS_LOG   8

#define LANE_COUNT  32  //  <--------------------------- Nvidia lanecount; change depending on hardware
#define LANE_MASK   31
#define LANE_LOG    5

//#define LANE_COUNT    64  <--------------------------- AMD
//#define LANE_MASK     63
//#define LANE_LOG      6
#define WARPS_PER_BLOCK (GROUP_SIZE >> LANE_LOG)

extern uint e_size;

#define FLAG_NOT_READY  0
#define FLAG_AGGREGATE  1
#define FLAG_PREFIX     2
#define FLAG_MASK       3

globallycoherent RWBuffer<uint> b_state;
RWBuffer<uint> b_prefixSum;

groupshared bool g_groupFlags[WARPS_PER_BLOCK];
groupshared uint g_agrMem[LANE_COUNT];

[numthreads(GROUP_SIZE, 1, 1)]
void Init(int3 id : SV_DispatchThreadID)
{
    for (int i = id.x; i < e_size; i += GROUP_SIZE * 32)
        b_prefixSum[i] = 1;
}

[numthreads(GROUP_SIZE, 1, 1)]
void InitPartitionDescriptors(int3 id : SV_DispatchThreadID)
{
    //Initialize the status flags
    if (id.x < THREAD_BLOCKS + 1)
        b_state[id.x] = FLAG_NOT_READY;
}

[numthreads(GROUP_SIZE, 1, 1)]
void ChainedDecoupledScanAtomic(int3 gtid : SV_GroupThreadID)
{
    //Atomically increment counter 
    int partitionIndex = 0;
    if (gtid.x == 0)
    {
        InterlockedAdd(b_state[THREAD_BLOCKS], 1, partitionIndex);
        for (int k = 0; k < WARPS_PER_BLOCK; k++)
            g_agrMem[k] = partitionIndex;
    }
    if (gtid.x < WARPS_PER_BLOCK)
        g_groupFlags[gtid.x] = true;
    AllMemoryBarrierWithGroupSync();
    partitionIndex = WaveReadLaneFirst(g_agrMem[gtid.x >> LANE_LOG]);
    
    int partitionSize = e_size >> THREAD_BLOCKS_LOG;
    int partitionStart = partitionIndex * partitionSize;
    int partitionEnd = partitionIndex == THREAD_BLOCKS - 1 ? e_size : (partitionIndex + 1) * partitionSize;
    partitionSize = partitionEnd - partitionStart;
    
    //aggregate
    int offset = 0;
    for (int j = partitionSize; j > 1; j >>= LANE_LOG)
    {
        for (int i = gtid.x; i < j; i += GROUP_SIZE)
        {
            const int t = ((i + 1) << offset) + partitionStart - 1;
            b_prefixSum[t] += WavePrefixSum(b_prefixSum[t]);
        }
        AllMemoryBarrierWithGroupSync();
        offset += LANE_LOG;
        
    }
    
    //Set flag payload
    if (gtid.x == 0)
    {
        if (partitionIndex == 0)
            InterlockedOr(b_state[partitionIndex], FLAG_PREFIX ^ (b_prefixSum[partitionEnd - 1] << 2));
        else
            InterlockedOr(b_state[partitionIndex], FLAG_AGGREGATE ^ (b_prefixSum[partitionEnd - 1] << 2));
    }
    
    //lookback
    int aggregate = 0;
    if (partitionIndex != 0)
    {
        int indexOffset = 0;
        do
        {
            if (gtid.x < LANE_COUNT)
            {
                //"Walk" down the partition aggregates
                for (int i = partitionIndex - (gtid.x + indexOffset + 1); 0 <= i; i -= LANE_COUNT)
                {
                    g_agrMem[gtid.x] = 0;
                    InterlockedOr(g_agrMem[gtid.x], b_state[i]);
                    int prefixIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((g_agrMem[gtid.x] & FLAG_MASK) == FLAG_PREFIX ? LANE_COUNT : 0));
                    int gapIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((g_agrMem[gtid.x] & FLAG_MASK) == FLAG_NOT_READY ? LANE_COUNT : 0));
                    if (prefixIndex < gapIndex)
                    {
                        aggregate += WaveActiveSum(gtid.x <= prefixIndex ? (g_agrMem[gtid.x] >> 2) : 0);
                        if (gtid.x == 0)
                        {
                            g_agrMem[gtid.x] = 0;
                            InterlockedOr(g_agrMem[gtid.x], b_prefixSum[partitionEnd - 1]);
                            InterlockedExchange(b_state[partitionIndex], ((aggregate + g_agrMem[gtid.x]) << 2) ^ FLAG_PREFIX, indexOffset);
                            for (int k = 0; k < WARPS_PER_BLOCK; k++)
                            {
                                g_groupFlags[k] = false;
                                g_agrMem[k] = aggregate;
                            }
                        }
                        break;
                    }
                    else
                    {
                        aggregate += WaveActiveSum(gtid.x < gapIndex ? (g_agrMem[gtid.x] >> 2) : 0);
                        indexOffset += gapIndex;
                        break;
                    }
                }
            }
            AllMemoryBarrierWithGroupSync();
        } while (WaveReadLaneFirst(g_groupFlags[gtid.x >> LANE_LOG]));
    }
    
    //Final sum, fan out aggregates
    aggregate = WaveReadLaneFirst(g_agrMem[gtid.x >> LANE_LOG]);
    offset = LANE_LOG;
    for (j = 1 << LANE_LOG; j < partitionSize; j <<= LANE_LOG)
    {
        for (int i = (gtid.x + 1 << offset); i < (j << LANE_LOG) && i < partitionSize; i += (GROUP_SIZE << offset))
            b_prefixSum[i + partitionStart - 1] += aggregate;
        AllMemoryBarrierWithGroupSync();
        
        for (i = gtid.x; i < partitionSize; i += GROUP_SIZE)
            if ((i & (j << LANE_LOG) - 1) >= j)
                if ((i + 1 & j - 1) != 0)
                    b_prefixSum[i + partitionStart] += b_prefixSum[((i >> offset) << offset) + partitionStart - 1];
        AllMemoryBarrierWithGroupSync();
        offset += LANE_LOG;
    }

    //Final aggregate propragation
    if (gtid.x < LANE_MASK)
        b_prefixSum[gtid.x + partitionStart] += aggregate;
    
    if (gtid.x == 0)
        b_prefixSum[partitionEnd - 1] += aggregate;
}