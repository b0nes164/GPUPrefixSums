/*******************************************************************************
 * Inclusive Chained Decoupled Lookback Scan Implementation
 * Author:  Thomas Smith 3/8/2023
 *
 * Variant: Warp-sized radix KoggeStone scans embedded into BrentKungBlelloch
 *          Global Memory
 *          Variable Partition Size
 *          Fixed ThreadBlocks/WorkGroups
 *          Fixed Partitions
 *          Atomic Loads
 *
 *
 * License: The Unlicense
 *          This is free and unencumbered software released into the public domain.
 *          For more information, please refer to the repository license or <https://unlicense.org>
 *   
 * Based off of Research by:
 *          Duane Merrill, Nvidia Corporation
 *          Michael Garland, Nvidia Corporation
 *          https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back
 *
 * This implementation does not constitute ANY form of endorsement or promotion
 * from its authors or Nvidia Corporation. In no event shall Nvidia Corporation
 * be held liable for ANY damages arising in any way out of the use of this 
 * software. The implementation author is not affiliated in ANY way with Nvidia
 * Corporation.
 *
 ******************************************************************************/
#pragma use_dxc
#pragma require Int64

#pragma kernel Init
#pragma kernel InitPartitionDescriptors
#pragma kernel ChainedDecoupledScanAtomic

#define GROUP_SIZE 1024
#define THREAD_BLOCKS 256
#define THREAD_BLOCKS_LOG 8

#define LANE_COUNT 32 //<--------------------------- Nvidia lanecount; change depending on hardware
#define LANE_MASK 31
#define LANE_LOG 5
#define WARPS_PER_BLOCK (GROUP_SIZE >> LANE_LOG)

//#define LANE_COUNT 64 <--------------------------- AMD lanecount
//#define LANE_MASK 63
//#define LANE_LOG 6

extern uint e_size;

#define FLAG_NOT_READY  0
#define FLAG_AGGREGATE  1
#define FLAG_PREFIX     2

globallycoherent RWBuffer<uint> b_state;
RWBuffer<uint> b_prefixSum;
RWBuffer<uint64_t> a;

groupshared bool g_groupFlags[WARPS_PER_BLOCK];
groupshared uint g_agrMem[LANE_COUNT];

[numthreads(GROUP_SIZE, 1, 1)]
void Init(int3 id : SV_DispatchThreadID)
{
    for (int i = id.x; i < e_size; i += GROUP_SIZE * 32)
        b_prefixSum[i] = 1;
}

[numthreads(GROUP_SIZE, 1, 1)]
void InitPartitionDescriptors(int3 id : SV_DispatchThreadID)
{
    //Initialize the status flags
    int locBlocks = ceil((THREAD_BLOCKS * 3.0f + 1) / GROUP_SIZE);
    for (int i = id.x; i < THREAD_BLOCKS * 3 + 1; i += GROUP_SIZE * locBlocks)
        b_state[i] = FLAG_NOT_READY;
}

//Fixed ThreadBlocks, Fixed Partitions, Variable Partition Size, Atomic loads
[numthreads(GROUP_SIZE, 1, 1)]
void ChainedDecoupledScanAtomic(int3 gtid : SV_GroupThreadID)
{
    //Atomically increment counter 
    int partitionIndex = 0;
    if (gtid.x == 0)
    {
        InterlockedAdd(b_state[THREAD_BLOCKS * 3], 1, partitionIndex);
        for (int k = 0; k < WARPS_PER_BLOCK; k++)
            g_agrMem[k] = partitionIndex;
    }
    if (gtid.x < WARPS_PER_BLOCK)
        g_groupFlags[gtid.x] = true;
    AllMemoryBarrierWithGroupSync();
    partitionIndex = WaveReadLaneFirst(g_agrMem[gtid.x >> LANE_LOG]);
    
    int partitionSize = e_size >> THREAD_BLOCKS_LOG;
    int partitionStart = partitionIndex * partitionSize;
    int partitionEnd = partitionIndex == THREAD_BLOCKS - 1 ? e_size : (partitionIndex + 1) * partitionSize;
    partitionSize = partitionEnd - partitionStart;
    
    //aggregate
    int offset = 0;
    for (int j = partitionSize; j > 1; j >>= LANE_LOG)
    {
        for (int i = gtid.x; i < j; i += GROUP_SIZE)
        {
            const int t = ((i + 1) << offset) + partitionStart - 1;
            b_prefixSum[t] += WavePrefixSum(b_prefixSum[t]);
        }
        AllMemoryBarrierWithGroupSync();
        offset += LANE_LOG;
        
    }
    
    //Set flags
    if (gtid.x == 0)
    {
        if (partitionIndex == 0)
        {
            InterlockedCompareStore(b_state[(THREAD_BLOCKS << 1) + partitionIndex], FLAG_NOT_READY, b_prefixSum[partitionEnd - 1]);
            InterlockedCompareStore(b_state[partitionIndex], FLAG_NOT_READY, FLAG_PREFIX);
        }
        else
        {
            InterlockedCompareStore(b_state[THREAD_BLOCKS + partitionIndex], FLAG_NOT_READY, b_prefixSum[partitionEnd - 1]);
            InterlockedCompareStore(b_state[partitionIndex], FLAG_NOT_READY, FLAG_AGGREGATE);
        }
    }
    
    //lookback
    int aggregate = 0;
    int counter = 0;
    if (partitionIndex != 0)
    {
        int indexOffset = 0;
        do
        {
            if (gtid.x < LANE_COUNT)
            {
                //"Walk" down the partition aggregates
                for (int i = partitionIndex - (gtid.x + indexOffset + 1); 0 <= i; i -= LANE_COUNT)
                {
                    uint t;
                    InterlockedCompareExchange(b_state[i], 0x7FFFFFFF, 0, t);
                    
                    int prefixIndex = WaveActiveMin(gtid.x + LANE_COUNT - (t == FLAG_PREFIX ? LANE_COUNT : 0));
                    int gapIndex = WaveActiveMin(gtid.x + LANE_COUNT - (t == FLAG_NOT_READY ? LANE_COUNT : 0));
                    if (prefixIndex < gapIndex)
                    {
                       
                        /*
                        InterlockedOr(b_state[gtid.x == prefixIndex ? (THREAD_BLOCKS << 1) + i : THREAD_BLOCKS + i], 0, t);
                        aggregate += WaveActiveSum(gtid.x <= prefixIndex ? t : 0);
                       */
                        
                        InterlockedCompareExchange(b_state[(THREAD_BLOCKS << 1) + i], 0x7FFFFFFF, 0, t);
                        aggregate += WaveActiveSum(gtid.x == prefixIndex ? t : 0);
                        
                        InterlockedCompareExchange(b_prefixSum[(i + 1) * partitionSize - 1], 0x7FFFFFFF, 0, t);
                        aggregate += WaveActiveSum(gtid.x < prefixIndex ? t : 0);
                        
                        if (gtid.x == 0)
                        {
                            InterlockedCompareExchange(b_prefixSum[partitionEnd - 1], 0x7FFFFFFF, 0, t);
                            InterlockedCompareStore(b_state[(THREAD_BLOCKS << 1) + partitionIndex], FLAG_NOT_READY, aggregate + t);
                            InterlockedCompareStore(b_state[partitionIndex], FLAG_AGGREGATE, FLAG_PREFIX);
                            for (int k = 0; k < WARPS_PER_BLOCK; k++)
                            {
                                g_groupFlags[k] = false;
                                g_agrMem[k] = aggregate;
                            }
                        }
                        break;
                    }
                    else
                    {
                        InterlockedCompareExchange(b_state[THREAD_BLOCKS + i], 0x7FFFFFFF, 0, t);
                        aggregate += WaveActiveSum(gtid.x < gapIndex ? t : 0);
                        indexOffset += gapIndex;
                        break;
                    }
                }
            }
            AllMemoryBarrierWithGroupSync();
        } while (WaveReadLaneFirst(g_groupFlags[gtid.x >> LANE_LOG]) && counter < 1000000);
    }
    
    //Final sum, fan out aggregates
    aggregate = WaveReadLaneFirst(g_agrMem[gtid.x >> LANE_LOG]);
    offset = LANE_LOG;
    for (j = 1 << LANE_LOG; j < partitionSize; j <<= LANE_LOG)
    {
        for (int i = (gtid.x + 1 << offset); i < (j << LANE_LOG) && i < partitionSize; i += (GROUP_SIZE << offset))
            b_prefixSum[i + partitionStart - 1] += aggregate;
        AllMemoryBarrierWithGroupSync();
        
        for (i = gtid.x; i < partitionSize; i += GROUP_SIZE)
            if ((i & (j << LANE_LOG) - 1) >= j)
                if ((i + 1 & j - 1) != 0)
                    b_prefixSum[i + partitionStart] += b_prefixSum[((i >> offset) << offset) + partitionStart - 1];
        AllMemoryBarrierWithGroupSync();
        offset += LANE_LOG;
    }

    //Final aggregate propragation
    if (gtid.x < LANE_MASK)
        b_prefixSum[gtid.x + partitionStart] += aggregate;
    
    if (gtid.x == 0)
        b_prefixSum[partitionEnd - 1] += aggregate;
}