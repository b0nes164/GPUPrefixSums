/******************************************************************************************************************
 * Chained Decoupled Scan Main Vectorized
 *
 * Scan:    Warp-sized radix raking reduce scan
 *
 * Variant: Raking warp-sized radix reduce using partitions of size equal to 
 *          maximum shared memory using vector loads and stores.
 *                    
 * Notes:   This variant uses the last bit of the first element in shared memory as a flag
 *          **Unrolls and preprocessor macros must be manually changed for AMD**
 * 
 * Author:  Thomas Smith 5/9/2023
 *   
 * Based off of Research by:
 *          Duane Merrill, Nvidia Corporation
 *          Michael Garland, Nvidia Corporation
 *          https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back
 *
 * Copyright (c) 2011, Duane Merrill.  All rights reserved.
 * Copyright (c) 2011-2022, NVIDIA CORPORATION.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *     * Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * Neither the name of the NVIDIA CORPORATION nor the
 *       names of its contributors may be used to endorse or promote products
 *       derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 ******************************************************************************/
#pragma use_dxc
#pragma kernel Init
#pragma kernel CD_Main_Vectorized
#pragma kernel CD_Main_Vectorized_Timing

#define PARTITION_SIZE      2048
#define PARTITION_MASK      2047
#define GROUP_SIZE          1024
#define THREAD_BLOCKS       256
#define PART_LOG            11

#define VECTOR_MASK         3
#define VECTOR_LOG          2

#define LANE_COUNT          32  // <---------------------------   For Nvidia; change depending on hardware
#define LANE_MASK           31
#define LANE_LOG            5
#define WAVES_PER_GROUP     32
#define WAVE_PARTITION_SIZE 64
#define WAVE_PART_LOG       6

//#define LANE_COUNT            64 <-------------------------   AMD 
//#define LANE_MASK             63
//#define LANE_LOG              6    
//#define WAVES_PER_GROUP       16
//#define WAVE_PARTITION_SIZE   128
//#define WAVE_PART_LOG         7

#define FLAG_NOT_READY  0
#define FLAG_AGGREGATE  1
#define FLAG_INCLUSIVE  2
#define FLAG_MASK       3

#define LANE            (gtid.x & LANE_MASK)
#define WAVE_INDEX      (gtid.x >> LANE_LOG)
#define SPINE_INDEX     (((gtid.x + 1) << WAVE_PART_LOG) - 1)
#define VECTOR_SIZE     ((e_size & VECTOR_MASK) ?       \
                        (e_size >> VECTOR_LOG) + 1 :    \
                        e_size >> VECTOR_LOG) 
#define PARTITIONS      ((VECTOR_SIZE & PARTITION_MASK) ?    \
                        (VECTOR_SIZE >> PART_LOG) + 1 :      \
                        VECTOR_SIZE >> PART_LOG )
#define WAVE_PART_START (WAVE_INDEX << WAVE_PART_LOG)
#define WAVE_PART_END   (WAVE_INDEX + 1 << WAVE_PART_LOG)
#define PARTITION_START (partitionIndex << PART_LOG)
#define EXACT_SIZE      (VECTOR_SIZE - PARTITION_START)
#define EXACT_WAVE_END  (WAVE_PART_END > EXACT_SIZE ? \
                        ((EXACT_SIZE - 1 >> WAVE_PART_LOG) == WAVE_INDEX ? EXACT_SIZE : 0) : \
                        WAVE_PART_END)

//Adjusted partition indexes for timing kernel
#define T_PART_INDEX        (partitionIndex & 32767)        //For 2^28 ONLY
#define T_PART_START        (T_PART_INDEX << PART_LOG)
#define T_EXACT_SIZE        (e_size - T_PART_START)
#define T_EXACT_WAVE_END    (WAVE_PART_END > T_EXACT_SIZE ? \
                            ((T_EXACT_SIZE - 1 >> WAVE_PART_LOG) == WAVE_INDEX ? T_EXACT_SIZE : 0) : \
                            WAVE_PART_END)

extern int e_size;
extern int e_repeats;

globallycoherent RWBuffer<uint> b_state;
RWStructuredBuffer<uint> b_prefixLoad;
RWBuffer<uint4> b_prefixSum;
groupshared uint4 g_sharedMem[PARTITION_SIZE];

[numthreads(GROUP_SIZE, 1, 1)]
void Init(int3 id : SV_DispatchThreadID)
{
    for (int i = id.x; i < e_size; i += GROUP_SIZE * THREAD_BLOCKS)
        b_prefixLoad[i] = 1;
}

[numthreads(GROUP_SIZE, 1, 1)]
void CD_Main_Vectorized(int3 gtid : SV_GroupThreadID)
{
    int partitionIndex;
    do
    {
        if (gtid.x == 0)
            InterlockedAdd(b_state[PARTITIONS], 1, g_sharedMem[0].x);
        GroupMemoryBarrierWithGroupSync();
        partitionIndex = WaveReadLaneFirst(g_sharedMem[0].x);
        GroupMemoryBarrierWithGroupSync();
        
        if (partitionIndex == PARTITIONS - 1)
        {
            const int wavePartEnd = EXACT_WAVE_END;
            
            int i = LANE + WAVE_PART_START;
            if(i < wavePartEnd)
            {
                g_sharedMem[i] = b_prefixSum[i + PARTITION_START];
                g_sharedMem[i].y += g_sharedMem[i].x;
                g_sharedMem[i].z += g_sharedMem[i].y;
                g_sharedMem[i].w += g_sharedMem[i].z;
                g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w);
            }
            
            i += LANE_COUNT;
            if(i < wavePartEnd)
            {
                g_sharedMem[i] = b_prefixSum[i + PARTITION_START];
                g_sharedMem[i].y += g_sharedMem[i].x;
                g_sharedMem[i].z += g_sharedMem[i].y;
                g_sharedMem[i].w += g_sharedMem[i].z;
                g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w) + WaveReadLaneFirst(g_sharedMem[i - 1].w);
            }
            GroupMemoryBarrierWithGroupSync();
        
            if (gtid.x < WAVES_PER_GROUP)
            {
                int t = SPINE_INDEX;
                if (t > EXACT_SIZE - 1)
                    t = (EXACT_SIZE - 1 >> WAVE_PART_LOG) == gtid.x ? EXACT_SIZE - 1 : PARTITION_SIZE - gtid.x;
                g_sharedMem[t] += WavePrefixSum(g_sharedMem[t].w);
            }
            GroupMemoryBarrierWithGroupSync();
            
            uint aggregate = 0;
            int indexOffset = 0;
            do
            {
                if (gtid.x < LANE_COUNT) //Restrict lookback to first warp
                {
                    for (int i = partitionIndex - (gtid.x + indexOffset + 1); 0 <= i; i -= LANE_COUNT)
                    {
                        uint flagPayload = b_state[i];
                        const int inclusiveIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE ? LANE_COUNT : 0));
                        const int gapIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_NOT_READY ? LANE_COUNT : 0));
                        if (inclusiveIndex < gapIndex)
                        {
                            aggregate += WaveActiveSum(gtid.x <= inclusiveIndex ? (flagPayload >> 2) : 0);
                            if (gtid.x == 0)
                            {
                                InterlockedOr(g_sharedMem[0].x, 0x80000000);
                                flagPayload = g_sharedMem[1].x;
                                g_sharedMem[1].x = aggregate;
                                aggregate = flagPayload;
                            }
                            break;
                        }
                        else
                        {
                            aggregate += WaveActiveSum(gtid.x < gapIndex ? (flagPayload >> 2) : 0);
                            indexOffset += gapIndex;
                            break;
                        }
                    }
                }
            } while ((WaveReadLaneFirst(g_sharedMem[0].x) & 0x80000000) == 0);
            GroupMemoryBarrierWithGroupSync();
            if (gtid.x == 0)
                g_sharedMem[0].x &= 0x7FFFFFFF;
            else
                aggregate = WaveReadLaneFirst(g_sharedMem[1].x);
            GroupMemoryBarrierWithGroupSync();
            if (gtid.x == 0)
                g_sharedMem[1].x = aggregate;
            if (gtid.x < 2)
                aggregate = WaveReadLaneAt(aggregate, 1);
            
            i = LANE + WAVE_PART_START;
            const uint prev = (WAVE_INDEX ? WaveReadLaneFirst(g_sharedMem[i - 1].w) : 0) + aggregate;
            if(i < wavePartEnd)
                b_prefixSum[i + PARTITION_START] = g_sharedMem[i] + (i < wavePartEnd - 1 ? prev : aggregate);
            i += LANE_COUNT;
            if (i < wavePartEnd)
                b_prefixSum[i + PARTITION_START] = g_sharedMem[i] + (i < wavePartEnd - 1 ? prev : aggregate);
        }
        else
        {
            int i = LANE + WAVE_PART_START;
            g_sharedMem[i] = b_prefixSum[i + PARTITION_START];
            g_sharedMem[i].y += g_sharedMem[i].x;
            g_sharedMem[i].z += g_sharedMem[i].y;
            g_sharedMem[i].w += g_sharedMem[i].z;
            g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w);
        
            i += LANE_COUNT;
            g_sharedMem[i] = b_prefixSum[i + PARTITION_START];
            g_sharedMem[i].y += g_sharedMem[i].x;
            g_sharedMem[i].z += g_sharedMem[i].y;
            g_sharedMem[i].w += g_sharedMem[i].z;
            g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w) + WaveReadLaneFirst(g_sharedMem[i - 1].w);
            GroupMemoryBarrierWithGroupSync();
        
            if (gtid.x < WAVES_PER_GROUP)
                g_sharedMem[SPINE_INDEX] += WavePrefixSum(g_sharedMem[SPINE_INDEX].w);
            GroupMemoryBarrierWithGroupSync();
            
            //Set flag payload
            if (gtid.x == 0)
            {
                if (partitionIndex == 0)
                    InterlockedOr(b_state[partitionIndex], FLAG_INCLUSIVE ^ (g_sharedMem[PARTITION_MASK].w << 2));
                else
                    InterlockedOr(b_state[partitionIndex], FLAG_AGGREGATE ^ (g_sharedMem[PARTITION_MASK].w << 2));
            }
            
            uint aggregate = 0;
            if (partitionIndex != 0)
            {
                int indexOffset = 0;
                do
                {
                    if (gtid.x < LANE_COUNT) //Restrict lookback to first warp
                    {
                        for (int i = partitionIndex - (gtid.x + indexOffset + 1); 0 <= i; i -= LANE_COUNT)
                        {
                            uint flagPayload = b_state[i];
                            const int inclusiveIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE ? LANE_COUNT : 0));
                            const int gapIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_NOT_READY ? LANE_COUNT : 0));
                            if (inclusiveIndex < gapIndex)
                            {
                                aggregate += WaveActiveSum(gtid.x <= inclusiveIndex ? (flagPayload >> 2) : 0);
                                if (gtid.x == 0)
                                {
                                    InterlockedAdd(b_state[partitionIndex], 1 | aggregate << 2);
                                    InterlockedOr(g_sharedMem[0].x, 0x80000000);
                                    flagPayload = g_sharedMem[1].x;
                                    g_sharedMem[1].x = aggregate;
                                    aggregate = flagPayload;
                                }
                                break;
                            }
                            else
                            {
                                aggregate += WaveActiveSum(gtid.x < gapIndex ? (flagPayload >> 2) : 0);
                                indexOffset += gapIndex;
                                break;
                            }
                        }
                    }
                } while ((WaveReadLaneFirst(g_sharedMem[0].x) & 0x80000000) == 0);
                GroupMemoryBarrierWithGroupSync();
            
                //propogate aggregate values
                if (gtid.x == 0)
                    g_sharedMem[0].x &= 0x7FFFFFFF;
                else
                    aggregate = WaveReadLaneFirst(g_sharedMem[1].x);
                GroupMemoryBarrierWithGroupSync();
                if (gtid.x == 0)
                    g_sharedMem[1].x = aggregate;
                if (gtid.x < 2)
                    aggregate = WaveReadLaneAt(aggregate, 1);
            }

            i = LANE + WAVE_PART_START;
            const uint prev = (WAVE_INDEX ? WaveReadLaneFirst(g_sharedMem[i - 1].w) : 0) + aggregate;
            b_prefixSum[i + PARTITION_START] = g_sharedMem[i] + prev;
            i += LANE_COUNT;
            b_prefixSum[i + PARTITION_START] = g_sharedMem[i] + (LANE != LANE_MASK ? prev : aggregate);
        }
    } while (partitionIndex + THREAD_BLOCKS < PARTITIONS);
}

/******************************************************************************
 * This is timing version of the scan. It is as similar as possible to the above
 * algorithm except that it can perform multiple loops. HOWEVER, IT IS NOT IDENTICAL,
 * and should only be interpreted as an approximation of the original algorithm. 
 ******************************************************************************/
[numthreads(GROUP_SIZE, 1, 1)]
void CD_Main_Vectorized_Timing(int3 gtid : SV_GroupThreadID)
{
    int partitionIndex;
    do
    {
        if (gtid.x == 0)
            InterlockedAdd(b_state[PARTITIONS * e_repeats], 1, g_sharedMem[0].x);
        GroupMemoryBarrierWithGroupSync();
        partitionIndex = WaveReadLaneFirst(g_sharedMem[0].x);
        GroupMemoryBarrierWithGroupSync();
        
        if (T_PART_INDEX == PARTITIONS - 1)
        {
            const int wavePartEnd = T_EXACT_WAVE_END;
            
            int i = LANE + WAVE_PART_START;
            if (i < wavePartEnd)
            {
                g_sharedMem[i] = b_prefixSum[i + T_PART_START];
                g_sharedMem[i].y += g_sharedMem[i].x;
                g_sharedMem[i].z += g_sharedMem[i].y;
                g_sharedMem[i].w += g_sharedMem[i].z;
                g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w);
            }
            
            i += LANE_COUNT;
            if (i < wavePartEnd)
            {
                g_sharedMem[i] = b_prefixSum[i + T_PART_START];
                g_sharedMem[i].y += g_sharedMem[i].x;
                g_sharedMem[i].z += g_sharedMem[i].y;
                g_sharedMem[i].w += g_sharedMem[i].z;
                g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w) + WaveReadLaneFirst(g_sharedMem[i - 1].w);
            }
            GroupMemoryBarrierWithGroupSync();
        
            if (gtid.x < WAVES_PER_GROUP)
            {
                int t = SPINE_INDEX;
                if (t > T_EXACT_SIZE - 1)
                    t = (T_EXACT_SIZE - 1 >> WAVE_PART_LOG) == gtid.x ? T_EXACT_SIZE - 1 : PARTITION_SIZE - gtid.x;
                g_sharedMem[t] += WavePrefixSum(g_sharedMem[t].w);
            }
            GroupMemoryBarrierWithGroupSync();
            
            uint aggregate = 0;
            int indexOffset = 0;
            do
            {
                if (gtid.x < LANE_COUNT) //Restrict lookback to first warp
                {
                    for (int i = partitionIndex - (gtid.x + indexOffset + 1); 0 <= i; i -= LANE_COUNT)
                    {
                        uint flagPayload = b_state[i];
                        const int inclusiveIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE ? LANE_COUNT : 0));
                        const int gapIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_NOT_READY ? LANE_COUNT : 0));
                        if (inclusiveIndex < gapIndex)
                        {
                            aggregate += WaveActiveSum(gtid.x <= inclusiveIndex ? (flagPayload >> 2) : 0);
                            if (gtid.x == 0)
                            {
                                InterlockedOr(g_sharedMem[0].x, 0x80000000);
                                flagPayload = g_sharedMem[1].x;
                                g_sharedMem[1].x = aggregate;
                                aggregate = flagPayload;
                            }
                            break;
                        }
                        else
                        {
                            aggregate += WaveActiveSum(gtid.x < gapIndex ? (flagPayload >> 2) : 0);
                            indexOffset += gapIndex;
                            break;
                        }
                    }
                }
            } while ((WaveReadLaneFirst(g_sharedMem[0].x) & 0x80000000) == 0);
            GroupMemoryBarrierWithGroupSync();
            if (gtid.x == 0)
                g_sharedMem[0].x &= 0x7FFFFFFF;
            else
                aggregate = WaveReadLaneFirst(g_sharedMem[1].x);
            GroupMemoryBarrierWithGroupSync();
            if (gtid.x == 0)
                g_sharedMem[1].x = aggregate;
            if (gtid.x < 2)
                aggregate = WaveReadLaneAt(aggregate, 1);
            
            i = LANE + WAVE_PART_START;
            const uint prev = (WAVE_INDEX ? WaveReadLaneFirst(g_sharedMem[i - 1].w) : 0) + aggregate;
            if (i < wavePartEnd)
                b_prefixSum[i + T_PART_START] = g_sharedMem[i] + (i < wavePartEnd - 1 ? prev : aggregate);
            i += LANE_COUNT;
            if (i < wavePartEnd)
                b_prefixSum[i + T_PART_START] = g_sharedMem[i] + (i < wavePartEnd - 1 ? prev : aggregate);
        }
        else
        {
            int i = LANE + WAVE_PART_START;
            g_sharedMem[i] = b_prefixSum[i + T_PART_START];
            g_sharedMem[i].y += g_sharedMem[i].x;
            g_sharedMem[i].z += g_sharedMem[i].y;
            g_sharedMem[i].w += g_sharedMem[i].z;
            g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w);
        
            i += LANE_COUNT;
            g_sharedMem[i] = b_prefixSum[i + T_PART_START];
            g_sharedMem[i].y += g_sharedMem[i].x;
            g_sharedMem[i].z += g_sharedMem[i].y;
            g_sharedMem[i].w += g_sharedMem[i].z;
            g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w) + WaveReadLaneFirst(g_sharedMem[i - 1].w);
            GroupMemoryBarrierWithGroupSync();
        
            if (gtid.x < WAVES_PER_GROUP)
                g_sharedMem[SPINE_INDEX] += WavePrefixSum(g_sharedMem[SPINE_INDEX].w);
            GroupMemoryBarrierWithGroupSync();
            
            if (gtid.x == 0)
            {
                if (T_PART_INDEX == 0)
                    InterlockedOr(b_state[partitionIndex], FLAG_INCLUSIVE ^ (g_sharedMem[PARTITION_MASK].w << 2));
                else
                    InterlockedOr(b_state[partitionIndex], FLAG_AGGREGATE ^ (g_sharedMem[PARTITION_MASK].w << 2));
            }
            
            uint aggregate = 0;
            if (T_PART_INDEX != 0)
            {
                int indexOffset = 0;
                do
                {
                    if (gtid.x < LANE_COUNT)
                    {
                        for (int i = partitionIndex - (gtid.x + indexOffset + 1); 0 <= i; i -= LANE_COUNT)
                        {
                            uint flagPayload = b_state[i];
                            const int inclusiveIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE ? LANE_COUNT : 0));
                            const int gapIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_NOT_READY ? LANE_COUNT : 0));
                            if (inclusiveIndex < gapIndex)
                            {
                                aggregate += WaveActiveSum(gtid.x <= inclusiveIndex ? (flagPayload >> 2) : 0);
                                if (gtid.x == 0)
                                {
                                    InterlockedAdd(b_state[partitionIndex], 1 | aggregate << 2);
                                    InterlockedOr(g_sharedMem[0].x, 0x80000000);
                                    flagPayload = g_sharedMem[1].x;
                                    g_sharedMem[1].x = aggregate;
                                    aggregate = flagPayload;
                                }
                                break;
                            }
                            else
                            {
                                aggregate += WaveActiveSum(gtid.x < gapIndex ? (flagPayload >> 2) : 0);
                                indexOffset += gapIndex;
                                break;
                            }
                        }
                    }
                } while ((WaveReadLaneFirst(g_sharedMem[0].x) & 0x80000000) == 0);
                GroupMemoryBarrierWithGroupSync();
            
                if (gtid.x == 0)
                    g_sharedMem[0].x &= 0x7FFFFFFF;
                else
                    aggregate = WaveReadLaneFirst(g_sharedMem[1].x);
                GroupMemoryBarrierWithGroupSync();
                if (gtid.x == 0)
                    g_sharedMem[1].x = aggregate;
                if (gtid.x < 2)
                    aggregate = WaveReadLaneAt(aggregate, 1);
            }

            i = LANE + WAVE_PART_START;
            const uint prev = (WAVE_INDEX ? WaveReadLaneFirst(g_sharedMem[i - 1].w) : 0) + aggregate;
            b_prefixSum[i + T_PART_START] = g_sharedMem[i] + prev;
            i += LANE_COUNT;
            b_prefixSum[i + T_PART_START] = g_sharedMem[i] + (LANE != LANE_MASK ? prev : aggregate);
        }
    } while (partitionIndex + THREAD_BLOCKS < PARTITIONS * e_repeats);
}