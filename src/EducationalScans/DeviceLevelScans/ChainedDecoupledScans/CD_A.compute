/******************************************************************************************************************
 * Chained Decoupled Scan A
 *
 * Variant: Reduce followed by warp-sized radix KoggeStone scans embedded into Brent Kung using
 *          partitions equally distributed among thread groups. 
 *
 * Notes:   Because the partition size will be greater than the maximum shared memory when
 *          the size of the sum exceeds THREAD_BLOCKS * 8192, this method has approximately
 *           ~3n global memory movement, and is thus is for educational purpose only.
 * 
 *          **Unrolls and preprocessor macros must be manually changed for AMD**
 *
 * Author:  Thomas Smith 5/9/2023
 *
 * License: The Unlicense
 *          This is free and unencumbered software released into the public domain.
 *          For more information, please refer to the repository license or <https://unlicense.org>
 *   
 * Based off of Research by:
 *          Duane Merrill, Corporation
 *          Michael Garland, Corporation
 *          https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back
 *
 * This implementation does not constitute ANY form of endorsement or promotion
 * from its authors or Nvidia Corporation. In no event shall Nvidia Corporation
 * be held liable for ANY damages arising in any way out of the use of this 
 * software. The implementation author is not affiliated in ANY way with Nvidia
 * Corporation.
 *
 ******************************************************************************/
#pragma use_dxc
#pragma kernel Init
#pragma kernel CD_A
#pragma kernel CD_A_Timing

#define SUB_PARTITION_SIZE  8192
#define SUB_PARTITION_MASK  8191
#define GROUP_SIZE          1024
#define THREAD_BLOCKS       256
#define SUB_PART_LOG        13
#define TBLOCK_LOG          8

#define LANE_COUNT          32  // <---------------------------   For Nvidia; change depending on hardware
#define LANE_MASK           31
#define LANE_LOG            5
#define WAVES_PER_GROUP     32
#define SUB_SUB_SIZE        1024
#define SUB_SUB_LOG         10
#define SUB_SUB_PARTITIONS  8

//#define LANE_COUNT            64 <-------------------------   AMD 
//#define LANE_MASK             63
//#define LANE_LOG              6    
//#define WAVES_PER_GROUP       16
//#define SUB_SUB_SIZE          4096
//#define SUB_SUB_LOG           12
//#define SUB_SUB_PARTITIONS    2

#define FLAG_NOT_READY      0
#define FLAG_AGGREGATE      1
#define FLAG_INCLUSIVE      2
#define FLAG_MASK           3

#define LANE                (gtid.x & LANE_MASK)
#define WAVE_INDEX          (gtid.x >> LANE_LOG)
#define SPINE_INDEX         (((gtid.x + 1) << LANE_LOG) - 1)
#define PARTITION_SIZE      (e_size >> TBLOCK_LOG)
#define PARTITION_START     (partitionIndex * PARTITION_SIZE)
#define EXACT_PART_SIZE     (partitionIndex == THREAD_BLOCKS - 1 ? \
                            e_size - (PARTITION_SIZE * partitionIndex) : PARTITION_SIZE)
#define SUB_PARTITIONS      ((EXACT_PART_SIZE & SUB_PARTITION_MASK) ? \
                            (EXACT_PART_SIZE >> SUB_PART_LOG) + 1 :   \
                            EXACT_PART_SIZE >> SUB_PART_LOG )
#define SUB_PART_START      (subPartitionIndex << SUB_PART_LOG)
#define EXACT_SUB_SIZE      (EXACT_PART_SIZE - SUB_PART_START)

//Adjusted partition indexes for timing kernel
#define T_PART_INDEX        (partitionIndex & 255)              //For 256 THREAD_BLOCKS ONLY
#define T_PART_START        (T_PART_INDEX * PARTITION_SIZE)
#define T_EXACT_PART_SIZE   (T_PART_INDEX == THREAD_BLOCKS - 1 ? \
                            e_size - (PARTITION_SIZE * (THREAD_BLOCKS - 1)) : PARTITION_SIZE)
#define T_SUB_PARTITIONS    ((T_EXACT_PART_SIZE & SUB_PARTITION_MASK) ? \
                            (T_EXACT_PART_SIZE >> SUB_PART_LOG) + 1 :   \
                            T_EXACT_PART_SIZE >> SUB_PART_LOG )
#define T_EXACT_SUB_SIZE    (T_EXACT_PART_SIZE - SUB_PART_START)

extern int e_size;
extern int e_repeats;

globallycoherent RWBuffer<uint> b_state;
RWBuffer<uint> b_prefixSum;
groupshared uint g_sharedMem[SUB_PARTITION_SIZE];

[numthreads(GROUP_SIZE, 1, 1)]
void Init(int3 id : SV_DispatchThreadID)
{
    for (int i = id.x; i < e_size; i += GROUP_SIZE * THREAD_BLOCKS)
        b_prefixSum[i] = 1;
}

[numthreads(GROUP_SIZE, 1, 1)]
void CD_A(int3 gtid : SV_GroupThreadID, int3 gid : SV_GroupID)
{
    if (gtid.x == 0)
        InterlockedAdd(b_state[THREAD_BLOCKS], 1, g_sharedMem[0]);
    GroupMemoryBarrierWithGroupSync();
    const int partitionIndex = WaveReadLaneFirst(g_sharedMem[0]);
    GroupMemoryBarrierWithGroupSync();

    uint aggregate = 0;
    for (int j = gtid.x; j < EXACT_PART_SIZE; j += GROUP_SIZE)
        aggregate += WaveActiveSum(b_prefixSum[j + PARTITION_START]);
    GroupMemoryBarrierWithGroupSync();
    
    if (LANE == 0)
        g_sharedMem[WAVE_INDEX] = aggregate;
    GroupMemoryBarrierWithGroupSync();
    
    //This will always be less than 1 warp
    if (gtid.x < WAVES_PER_GROUP)
        g_sharedMem[gtid.x] = WaveActiveSum(g_sharedMem[gtid.x]);
    
    if (gtid.x == 0)
    {
        if (partitionIndex == 0)
            InterlockedOr(b_state[partitionIndex], FLAG_INCLUSIVE ^ (g_sharedMem[1] << 2));
        else
        {
            InterlockedOr(b_state[partitionIndex], FLAG_AGGREGATE ^ (g_sharedMem[1] << 2));
            g_sharedMem[0] = 0;
        }
    }
    GroupMemoryBarrierWithGroupSync();
    
    aggregate = 0;
    if (partitionIndex != 0)
    {
        int indexOffset = 0;
        do
        {
            if (gtid.x < LANE_COUNT)
            {
                for (int i = partitionIndex - (gtid.x + indexOffset + 1); 0 <= i; )
                {
                    uint flagPayload = b_state[i];
                    const int inclusiveIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE ? LANE_COUNT : 0));
                    const int gapIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_NOT_READY ? LANE_COUNT : 0));
                    if (inclusiveIndex < gapIndex)
                    {
                        aggregate += WaveActiveSum(gtid.x <= inclusiveIndex ? (flagPayload >> 2) : 0);
                        if (gtid.x == 0)
                        {
                            InterlockedAdd(b_state[partitionIndex], 1 | aggregate << 2);
                            InterlockedOr(g_sharedMem[0], aggregate);
                        }
                        break;
                    }
                    else
                    {
                        aggregate += WaveActiveSum(gtid.x < gapIndex ? (flagPayload >> 2) : 0);
                        indexOffset += gapIndex;
                        break;
                    }
                }
            }
        } while (WaveReadLaneFirst(g_sharedMem[0]) == 0);
        GroupMemoryBarrierWithGroupSync();
        
        aggregate = WaveReadLaneFirst(g_sharedMem[0]);
        GroupMemoryBarrierWithGroupSync();
    }

    for (int subPartitionIndex = 0; subPartitionIndex < SUB_PARTITIONS - 1; ++subPartitionIndex)
    {
        [unroll(8)]
        for (int i = gtid.x; i < SUB_PARTITION_SIZE; i += GROUP_SIZE)
        {
            g_sharedMem[i] = b_prefixSum[i + PARTITION_START + SUB_PART_START];
            g_sharedMem[i] += WavePrefixSum(g_sharedMem[i]);
        }
        GroupMemoryBarrierWithGroupSync();
        
        if (gtid.x < (SUB_PARTITION_SIZE >> LANE_LOG))
            g_sharedMem[SPINE_INDEX] += WavePrefixSum(g_sharedMem[SPINE_INDEX]);
        GroupMemoryBarrierWithGroupSync();
        
        for (int j = 0; j < SUB_SUB_PARTITIONS; ++j)
        {
            const int t = gtid.x + (j << SUB_SUB_LOG);
            b_prefixSum[t + PARTITION_START + SUB_PART_START] = g_sharedMem[t] + aggregate + 
                ((t + 1 & LANE_MASK) && WAVE_INDEX ? WaveReadLaneFirst(g_sharedMem[t - 1]) : 0);
            
            aggregate += WaveReadLaneFirst(g_sharedMem[(j + 1 << SUB_SUB_LOG) - 1]);
        }
    }
    
    for (int i = gtid.x; i < EXACT_SUB_SIZE; i += GROUP_SIZE)
    {
        g_sharedMem[i] = b_prefixSum[i + PARTITION_START + SUB_PART_START];
        g_sharedMem[i] += WavePrefixSum(g_sharedMem[i]);
    }
    GroupMemoryBarrierWithGroupSync();
    
    if (gtid.x < (EXACT_SUB_SIZE >> LANE_LOG))
        g_sharedMem[SPINE_INDEX] += WavePrefixSum(g_sharedMem[SPINE_INDEX]);
    GroupMemoryBarrierWithGroupSync();
    
    for (int j = 0; j < SUB_SUB_PARTITIONS; ++j)
    {
        const int t = gtid.x + (j << SUB_SUB_LOG);
        if (t < EXACT_SUB_SIZE)
        {
            b_prefixSum[t + PARTITION_START + SUB_PART_START] = g_sharedMem[t] + aggregate +
                ((t + 1 & LANE_MASK) && WAVE_INDEX ? WaveReadLaneFirst(g_sharedMem[t - 1]) : 0);
            
            aggregate += WaveReadLaneFirst(g_sharedMem[(j + 1 << SUB_SUB_LOG) - 1]);
        }
    }
}

/******************************************************************************
 * This is timing version of the scan. It is as similar as possible to the above
 * algorithm except that it can perform multiple loops. HOWEVER, IT IS NOT IDENTICAL,
 * and should only be interpreted as an approximation of the original algorithm. 
 ******************************************************************************/
[numthreads(GROUP_SIZE, 1, 1)]
void CD_A_Timing(int3 gtid : SV_GroupThreadID)
{
    int partitionIndex;
    do
    {
        if (gtid.x == 0)
            InterlockedAdd(b_state[THREAD_BLOCKS * e_repeats], 1, g_sharedMem[0]);
        GroupMemoryBarrierWithGroupSync();
        partitionIndex = WaveReadLaneFirst(g_sharedMem[0]);
        GroupMemoryBarrierWithGroupSync();

        uint aggregate = 0;
        for (int j = gtid.x; j < T_EXACT_PART_SIZE; j += GROUP_SIZE)
            aggregate += WaveActiveSum(b_prefixSum[j + T_PART_START]);
        GroupMemoryBarrierWithGroupSync();
    
        if (LANE == 0)
            g_sharedMem[WAVE_INDEX] = aggregate;
        GroupMemoryBarrierWithGroupSync();
    
        //This will always be less than 1 warp
        if (gtid.x < WAVES_PER_GROUP)
            g_sharedMem[gtid.x] = WaveActiveSum(g_sharedMem[gtid.x]);
    
        if (gtid.x == 0)
        {
            if (partitionIndex == 0)
                InterlockedOr(b_state[partitionIndex], FLAG_INCLUSIVE ^ (g_sharedMem[1] << 2));
            else
            {
                InterlockedOr(b_state[partitionIndex], FLAG_AGGREGATE ^ (g_sharedMem[1] << 2));
                g_sharedMem[0] = 0;
            }
        }
        GroupMemoryBarrierWithGroupSync();
    
        aggregate = 0;
        if (partitionIndex != 0)
        {
            int indexOffset = 0;
            do
            {
                if (gtid.x < LANE_COUNT)
                {
                    for (int i = partitionIndex - (gtid.x + indexOffset + 1); 0 <= i; )
                    {
                        uint flagPayload = b_state[i];
                        const int inclusiveIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE ? LANE_COUNT : 0));
                        const int gapIndex = WaveActiveMin(gtid.x + LANE_COUNT - ((flagPayload & FLAG_MASK) == FLAG_NOT_READY ? LANE_COUNT : 0));
                        if (inclusiveIndex < gapIndex)
                        {
                            aggregate += WaveActiveSum(gtid.x <= inclusiveIndex ? (flagPayload >> 2) : 0);
                            if (gtid.x == 0)
                            {
                                InterlockedAdd(b_state[partitionIndex], 1 | aggregate << 2);
                                InterlockedOr(g_sharedMem[0], aggregate);
                            }
                            break;
                        }
                        else
                        {
                            aggregate += WaveActiveSum(gtid.x < gapIndex ? (flagPayload >> 2) : 0);
                            indexOffset += gapIndex;
                            break;
                        }
                    }
                }
            } while (WaveReadLaneFirst(g_sharedMem[0]) == 0);
            GroupMemoryBarrierWithGroupSync();
        
            aggregate = WaveReadLaneFirst(g_sharedMem[0]);
            GroupMemoryBarrierWithGroupSync();
        }

        for (int subPartitionIndex = 0; subPartitionIndex < T_SUB_PARTITIONS - 1; ++subPartitionIndex)
        {
            [unroll(8)]
            for (int i = gtid.x; i < SUB_PARTITION_SIZE; i += GROUP_SIZE)
            {
                g_sharedMem[i] = b_prefixSum[i + T_PART_START + SUB_PART_START];
                g_sharedMem[i] += WavePrefixSum(g_sharedMem[i]);
            }
            GroupMemoryBarrierWithGroupSync();
        
            if (gtid.x < (SUB_PARTITION_SIZE >> LANE_LOG))
                g_sharedMem[SPINE_INDEX] += WavePrefixSum(g_sharedMem[SPINE_INDEX]);
            GroupMemoryBarrierWithGroupSync();
        
            for (int j = 0; j < SUB_SUB_PARTITIONS; ++j)
            {
                const int t = gtid.x + (j << SUB_SUB_LOG);
                b_prefixSum[t + T_PART_START + SUB_PART_START] = g_sharedMem[t] + aggregate +
                ((t + 1 & LANE_MASK) && WAVE_INDEX ? WaveReadLaneFirst(g_sharedMem[t - 1]) : 0);
            
                aggregate += WaveReadLaneFirst(g_sharedMem[(j + 1 << SUB_SUB_LOG) - 1]);
            }
        }
    
        for (int i = gtid.x; i < T_EXACT_PART_SIZE; i += GROUP_SIZE)
        {
            g_sharedMem[i] = b_prefixSum[i + T_PART_START + SUB_PART_START];
            g_sharedMem[i] += WavePrefixSum(g_sharedMem[i]);
        }
        GroupMemoryBarrierWithGroupSync();
    
        if (gtid.x < (T_EXACT_PART_SIZE >> LANE_LOG))
            g_sharedMem[SPINE_INDEX] += WavePrefixSum(g_sharedMem[SPINE_INDEX]);
        GroupMemoryBarrierWithGroupSync();
        
        for (int j = 0; j < SUB_SUB_PARTITIONS; ++j)
        {
            const int t = gtid.x + (j << SUB_SUB_LOG);
            if (t < T_EXACT_SUB_SIZE)
            {
                b_prefixSum[t + T_PART_START + SUB_PART_START] = g_sharedMem[t] + aggregate +
                ((t + 1 & LANE_MASK) && WAVE_INDEX ? WaveReadLaneFirst(g_sharedMem[t - 1]) : 0);
            
                    aggregate += WaveReadLaneFirst(g_sharedMem[(j + 1 << SUB_SUB_LOG) - 1]);
            }
        }
    } while (partitionIndex + THREAD_BLOCKS < (e_repeats << TBLOCK_LOG));
}
