/*******************************************************************************
 * Prefix Sum Survey
 * Author:  Thomas Smith 5/12/2023
 *
 * License: The Unlicense
 * This is free and unencumbered software released into the public domain.
 * For more information, please refer to the repository license or <https://unlicense.org>
 *
 ******************************************************************************/
#pragma use_dxc
#pragma kernel Init
#pragma kernel Serial
#pragma kernel KoggeStoneWarp
#pragma kernel WarpIntrinsic
#pragma kernel RakingReduce
#pragma kernel RadixRakingReduce
#pragma kernel KoggeStone
#pragma kernel Sklansky
#pragma kernel BrentKung
#pragma kernel BrentKungBlelloch
#pragma kernel ReduceScan
#pragma kernel RadixBrentKung
#pragma kernel RadixSklansky
#pragma kernel BrentKungLarge
#pragma kernel BrentKungBlellochLarge
#pragma kernel BrentKungLargeUnrolled
#pragma kernel ReduceScanLarge
#pragma kernel RadixBrentKungLarge
#pragma kernel RadixBrentKungFused
#pragma kernel RadixSklanskyLarge
#pragma kernel RadixSklanskyAdvanced

#define GROUP_SIZE          1024
#define SHARED_MEMORY_SIZE  8192 // This is the maximum size in HLSL 32kb

#define LANE_COUNT  32  //  <--------------------------- Nvidia lanecount; change depending on hardware
#define LANE_MASK   31
#define LANE_LOG    5

//#define LANE_COUNT    64  <--------------------------- AMD lanecount
//#define LANE_MASK     63
//#define LANE_LOG      6

extern uint e_size;
RWBuffer<uint> prefixSumBuffer;
groupshared uint g_reduceValues[SHARED_MEMORY_SIZE];

[numthreads(GROUP_SIZE, 1, 1)]
void Init(int3 id : SV_DispatchThreadID)
{
    for (int i = id.x; i < e_size; i += GROUP_SIZE * 256)
        prefixSumBuffer[i] = 1;
}

[numthreads(1, 1, 1)]
void Serial(int3 gtid : SV_GroupThreadID)
{
    for(uint i = 1; i < e_size; ++i)
        prefixSumBuffer[i] += prefixSumBuffer[i - 1];
}

/***********************************************************************************
* Warp-level sums of size LESS THAN OR EQUAL TO to the WARP size.
************************************************************************************/
[numthreads(LANE_COUNT, 1, 1)]
void KoggeStoneWarp(int3 gtid : SV_GroupThreadID)
{
    if(gtid.x < e_size)
    {
        if(gtid.x > 0) prefixSumBuffer[gtid.x] += prefixSumBuffer[gtid.x - 1];
        if(gtid.x > 1) prefixSumBuffer[gtid.x] += prefixSumBuffer[gtid.x - 2];
        if(gtid.x > 3) prefixSumBuffer[gtid.x] += prefixSumBuffer[gtid.x - 4];
        if(gtid.x > 7) prefixSumBuffer[gtid.x] += prefixSumBuffer[gtid.x - 8];
        if(gtid.x > 15) prefixSumBuffer[gtid.x] += prefixSumBuffer[gtid.x - 16];
    }    
}

[numthreads(LANE_COUNT, 1, 1)]
void WarpIntrinsic(int3 gtid : SV_GroupThreadID)
{
    if(gtid.x < e_size)
        prefixSumBuffer[gtid.x] += WavePrefixSum(prefixSumBuffer[gtid.x]);
}

/***********************************************************************************
* Warp-level sums of size GREATER THAN OR EQUAL TO to the WARP size.
************************************************************************************/
[numthreads(LANE_COUNT, 1, 1)]
void RakingReduce(int3 gtid : SV_GroupThreadID)
{
    const int partitionSize = e_size >> LANE_LOG;
    const int partStart = partitionSize * gtid.x;
    const int partEnd = (gtid.x + 1) * partitionSize - 1;
    
    //Per-thread serial reductions
    for (int j = partStart + 1; j <= partEnd; ++j)
        prefixSumBuffer[j] += prefixSumBuffer[j - 1];
    
    //Single Kogge-Stone on the aggregates
    prefixSumBuffer[partEnd] += WavePrefixSum(prefixSumBuffer[partEnd]);
    
    //Per-thread serial propogation
    if (gtid.x > 0)
        for (j = partStart; j < partEnd; ++j)
            prefixSumBuffer[j] += prefixSumBuffer[partStart - 1];
}

[numthreads(LANE_COUNT, 1, 1)]
void RadixRakingReduce(int3 gtid : SV_GroupThreadID)
{
    const int partitions = e_size >> LANE_LOG;
    
    //First kogge-stone warp scan without passing in passing in aggregate
    prefixSumBuffer[gtid.x] += WavePrefixSum(prefixSumBuffer[gtid.x]);
    
    //Walk up partitions, passing in the agrregate as we go
    for (int partitionIndex = 1; partitionIndex < partitions; ++partitionIndex)
    {
        const int partitionStart = partitionIndex << LANE_LOG;
        const int t = gtid.x + partitionStart;
        prefixSumBuffer[t] += WavePrefixSum(prefixSumBuffer[t]) + prefixSumBuffer[partitionStart - 1];
    }
}

/***********************************************************************************
* Block-level sums of size LESS THAN OR EQUAL TO to the GROUP size.
************************************************************************************/
[numthreads(GROUP_SIZE, 1, 1)]
void KoggeStone(int3 gtid : SV_GroupThreadID)
{
    for (int j = 1; j < e_size; j <<= 1)
    {
        if (gtid.x + j < e_size)
            prefixSumBuffer[gtid.x + j] += prefixSumBuffer[gtid.x];
        DeviceMemoryBarrierWithGroupSync();
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void Sklansky(int3 gtid : SV_GroupThreadID)
{
    int offset = 0;
    for (int j = 1; j < e_size; j <<= 1)
    {
        if ((gtid.x & j) != 0 && gtid.x < e_size)
            prefixSumBuffer[gtid.x] += prefixSumBuffer[((gtid.x >> offset) << offset) - 1];
        DeviceMemoryBarrierWithGroupSync();
        ++offset;
    }
}

//my own implementation
[numthreads(GROUP_SIZE, 1, 1)]
void BrentKung(int3 gtid : SV_GroupThreadID)
{
    //Upsweep
    int offset = 1;
    for (int j = 1; j < e_size; j <<= 1)
    {
        const int t = ((gtid.x + 1) << offset) - 1;
        if (t < e_size)
            prefixSumBuffer[t] += prefixSumBuffer[t - j];
        DeviceMemoryBarrierWithGroupSync();
        ++offset;
    }
    
    //Downsweep
    --offset;
    for (int j = (e_size >> 2); j > 0; j >>= 1)
    {
        --offset;
        const int t = ((gtid.x + 1) << offset) + j - 1;
        if (t < e_size)
            prefixSumBuffer[t] += prefixSumBuffer[t - j];
        DeviceMemoryBarrierWithGroupSync();
    }
}

//the classic
[numthreads(GROUP_SIZE, 1, 1)]
void BrentKungBlelloch(int3 gtid : SV_GroupThreadID)
{
    //Upsweep
    if (gtid.x < (e_size >> 1))
        prefixSumBuffer[(gtid.x << 1) + 1] += prefixSumBuffer[gtid.x << 1];
    
    int offset = 1;
    for (int j = e_size >> 2; j > 0; j >>= 1)
    {
        DeviceMemoryBarrierWithGroupSync();
        if (gtid.x < j)
            prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 1) << offset) - 1];
        ++offset;
    }
    //Downsweep
    for (j = 1; j < e_size; j <<= 1)
    {
        --offset;
        DeviceMemoryBarrierWithGroupSync();
        if (gtid.x < j)
            prefixSumBuffer[(((gtid.x << 1) + 3) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1];
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void ReduceScan(int3 gtid : SV_GroupThreadID)
{
    //cant be less than 2
    int spillFactor = 4;
    int spillSize = e_size >> spillFactor;
    
    //Upsweep until desired threshold
    if (gtid.x < (e_size >> 1))
        prefixSumBuffer[(gtid.x << 1) + 1] += prefixSumBuffer[(gtid.x << 1)];
    AllMemoryBarrierWithGroupSync();
    
    int offset = 1;
    for (int j = e_size >> 2; j > spillSize; j >>= 1)
    {
        if (gtid.x < j)
            prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 1) << offset) - 1];
        AllMemoryBarrierWithGroupSync();
        ++offset;
    }
    
    //Pass intermediates into secondary buffer
    if (gtid.x < j)
    {
        const int t = (((gtid.x << 1) + 2) << offset) - 1;
        g_reduceValues[gtid.x] = prefixSumBuffer[t] + prefixSumBuffer[(((gtid.x << 1) + 1) << offset) - 1];
        prefixSumBuffer[t] = g_reduceValues[gtid.x];
    }
    AllMemoryBarrierWithGroupSync();
    
    //Reduce intermediates
    offset = 0;
    for (j = 1; j < spillSize; j <<= 1)
    {
        if ((gtid.x & j) != 0 && gtid.x < spillSize)
            g_reduceValues[gtid.x] += g_reduceValues[((gtid.x >> offset) << offset) - 1];
        AllMemoryBarrierWithGroupSync();
        ++offset;
    }
    
    //Pass in intermediates and downsweep
    offset = spillFactor - 2;
    const int t = (((gtid.x << 1) + 2) << offset) + (1 << offset + 1) - 1;
    if (t  < e_size)
        InterlockedAdd(prefixSumBuffer[t], g_reduceValues[(t >> spillFactor) - 1]);
    
    for (j = spillSize << 1; j < e_size; j <<= 1)
    {
        AllMemoryBarrierWithGroupSync();
        if (gtid.x < j)
            prefixSumBuffer[(((gtid.x << 1) + 3) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1];
        offset--;
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void RadixBrentKung(int3 gtid : SV_GroupThreadID)
{
    //Perform warp-sized KoggeStone reductions
    if (gtid.x < e_size)
        prefixSumBuffer[gtid.x] += WavePrefixSum(prefixSumBuffer[gtid.x]);
    DeviceMemoryBarrierWithGroupSync();
    
    //Warp-sized BrentKung upsweep
    const int temp = ((gtid.x + 1) << LANE_LOG) - 1;
    if (gtid.x < (e_size >> LANE_LOG))
        prefixSumBuffer[temp] += WavePrefixSum(prefixSumBuffer[temp]);
    DeviceMemoryBarrierWithGroupSync();
    
    //Fan the aggregates from the upsweep
    const int lane = gtid.x & LANE_MASK;
    const int t = gtid.x + LANE_COUNT;
    if (lane != LANE_MASK && t < e_size)
        prefixSumBuffer[t] += prefixSumBuffer[((t >> LANE_LOG) << LANE_LOG) - 1];
}

[numthreads(GROUP_SIZE, 1, 1)]
void RadixSklansky(int3 gtid : SV_GroupThreadID)
{
    //Perform warp-sized KoggeStone reductions
    if (gtid.x < e_size)
        prefixSumBuffer[gtid.x] += WavePrefixSum(prefixSumBuffer[gtid.x]);
    DeviceMemoryBarrierWithGroupSync();
    
    //Warp-sized Sklansky propogation fan
    int offset = LANE_LOG;
    for (int j = 1 << LANE_LOG; j < e_size; j <<= 1)
    {
        for (int i = gtid.x; i < e_size; i += GROUP_SIZE)
            if ((i & j) != 0)
                prefixSumBuffer[i] += prefixSumBuffer[((i >> offset) << offset) - 1];
        DeviceMemoryBarrierWithGroupSync();
        ++offset;
    }
}

/***********************************************************************************
* Block-level sums of size GREATER THAN the block/group size.
************************************************************************************/
//my own implementation, unfortunately breaks at size of 2^21 due to integer overflowing
[numthreads(GROUP_SIZE, 1, 1)]
void BrentKungLarge(int3 gtid : SV_GroupThreadID)
{
    //upsweep
    int offset = 1;
    for (int j = 1; j < e_size; j <<= 1)
    {
        for (int i = ((gtid.x + 1) << offset) - 1; i < e_size; i += GROUP_SIZE << offset)
            prefixSumBuffer[i] += prefixSumBuffer[i - j];
        DeviceMemoryBarrierWithGroupSync();
        ++offset;
    }
    
    //Downsweep
    --offset;
    for (j = (e_size >> 2); j > 0; j >>= 1)
    {
        --offset;
        for (int i = ((gtid.x + 1) << offset) + j - 1; i < e_size; i += GROUP_SIZE << offset)
            prefixSumBuffer[i] += prefixSumBuffer[i - j];
        DeviceMemoryBarrierWithGroupSync();
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void BrentKungBlellochLarge(int3 gtid : SV_GroupThreadID)
{
    //Upsweep
    for(int j = gtid.x; j < (e_size >> 1); j += GROUP_SIZE)
        prefixSumBuffer[(j << 1) + 1] += prefixSumBuffer[j << 1];
    
    int offset = 1;
    for (int j = e_size >> 2; j > 0; j >>= 1)
    {
        DeviceMemoryBarrierWithGroupSync();
        for (int i = gtid.x; i < j; i += GROUP_SIZE)
            prefixSumBuffer[(((i << 1) + 2) << offset) - 1] += prefixSumBuffer[(((i << 1) + 1) << offset) - 1];
        ++offset;
    }
    
    //Downsweep
    for (int j = 1; j < e_size; j <<= 1)
    {
        --offset;
        DeviceMemoryBarrierWithGroupSync();
        for (int i = gtid.x; i < j; i += GROUP_SIZE)
            prefixSumBuffer[(((i << 1) + 3) << offset) - 1] += prefixSumBuffer[(((i << 1) + 2) << offset) - 1];
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void BrentKungLargeUnrolled(int3 gtid : SV_GroupThreadID)
{
    if ((e_size >> 1) > LANE_COUNT)
    {
        for (int j = gtid.x; j < (e_size >> 1); j += GROUP_SIZE)
            prefixSumBuffer[(j << 1) + 1] += prefixSumBuffer[j << 1];
        DeviceMemoryBarrierWithGroupSync();
    }

    int offset = (e_size >> 1) > LANE_COUNT ? 1 : 0;
    for (int j = e_size >> 2; j > LANE_COUNT; j >>= 1)
    {
        for (int i = gtid.x; i < j; i += GROUP_SIZE)
            prefixSumBuffer[(((i << 1) + 2) << offset) - 1] += prefixSumBuffer[(((i << 1) + 1) << offset) - 1];
        DeviceMemoryBarrierWithGroupSync();
        ++offset;
    }
    
    //Unroll the final passes of the upsweep, and the initial passes of the downsweep.
    if (gtid.x < LANE_COUNT)
    {
        prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 1) << offset) - 1];
        ++offset;
        if (gtid.x < 16)
            prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 1) << offset) - 1];
        ++offset;
        if (gtid.x < 8)
            prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 1) << offset) - 1];
        ++offset;
        if (gtid.x < 4)
            prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 1) << offset) - 1];
        ++offset;
        if (gtid.x < 2)
            prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 1) << offset) - 1];
        ++offset;
        
        if (gtid.x < 1)
        {
            prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 1) << offset) - 1];
            prefixSumBuffer[(((gtid.x << 1) + 3) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1];
        }
        
        --offset;
        if (gtid.x < 2)
            prefixSumBuffer[(((gtid.x << 1) + 3) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1];
        --offset;
        if (gtid.x < 4)
            prefixSumBuffer[(((gtid.x << 1) + 3) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1];
        --offset;
        if (gtid.x < 8)
            prefixSumBuffer[(((gtid.x << 1) + 3) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1];
        --offset;
        if (gtid.x < 16)
            prefixSumBuffer[(((gtid.x << 1) + 3) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1];
        --offset;
        if (gtid.x < 32)
            prefixSumBuffer[(((gtid.x << 1) + 3) << offset) - 1] += prefixSumBuffer[(((gtid.x << 1) + 2) << offset) - 1];
    }
    
    //Downsweep
    for (int j = (LANE_COUNT << 1); j < e_size; j <<= 1)
    {
        --offset;
        DeviceMemoryBarrierWithGroupSync();
        for (int i = gtid.x; i < j; i += GROUP_SIZE)
            prefixSumBuffer[(((i << 1) + 3) << offset) - 1] += prefixSumBuffer[(((i << 1) + 2) << offset) - 1];
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void ReduceScanLarge(int3 gtid : SV_GroupThreadID)
{
    //cant be less than 2
    int spillFactor = 5;
    int spillSize = e_size >> spillFactor;
    
    //Upsweep until desired threshold
    for (int j = gtid.x; j < (e_size >> 1); j += GROUP_SIZE)
        prefixSumBuffer[(j << 1) + 1] += prefixSumBuffer[j << 1];
    AllMemoryBarrierWithGroupSync();
    
    int offset = 1;
    for (int j = e_size >> 2; j > spillSize; j >>= 1)
    {
        for (int i = gtid.x; i < j; i += GROUP_SIZE)
            prefixSumBuffer[(((i << 1) + 2) << offset) - 1] += prefixSumBuffer[(((i << 1) + 1) << offset) - 1];
        AllMemoryBarrierWithGroupSync();
        ++offset;
    }
    
    //Pass intermediates into secondary buffer
    for(int k = gtid.x; k < j; k += GROUP_SIZE)
    {
        const int t = (((k << 1) + 2) << offset) - 1;
        g_reduceValues[k] = prefixSumBuffer[t] + prefixSumBuffer[(((k << 1) + 1) << offset) - 1];
        prefixSumBuffer[t] = g_reduceValues[k];
    }
    AllMemoryBarrierWithGroupSync();
    
    //Reduce intermediates using radix sklansky reduction
    for (int i = gtid.x; i < spillSize; i += GROUP_SIZE)
        g_reduceValues[i] += WavePrefixSum(g_reduceValues[i]);
    AllMemoryBarrierWithGroupSync();
    
    offset = LANE_LOG;
    for (int j = 1 << LANE_LOG; j < spillSize; j <<= 1)
    {
        for (int i = gtid.x; i < spillSize; i += GROUP_SIZE)
            if ((i & j) != 0)
                g_reduceValues[i] += g_reduceValues[((i >> offset) << offset) - 1];
        AllMemoryBarrierWithGroupSync();
        ++offset;
    }
    
    //Pass in intermediates and downsweep
    offset = spillFactor - 2;
    for (int k = (((gtid.x << 1) + 2) << offset) + (1 << offset + 1) - 1; k < e_size; k += GROUP_SIZE << offset + 1)
        InterlockedAdd(prefixSumBuffer[k], g_reduceValues[(k >> spillFactor) - 1]);
    
    //Downsweep
    for (int j = spillSize << 1; j < e_size; j <<= 1)
    { 
        AllMemoryBarrierWithGroupSync();
        for (int i = gtid.x; i < j; i += GROUP_SIZE)
            prefixSumBuffer[(((i << 1) + 3) << offset) - 1] += prefixSumBuffer[(((i << 1) + 2) << offset) - 1];
        --offset;
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void RadixBrentKungLarge(int3 gtid : SV_GroupThreadID)
{
    //Upsweep
    //Warp-sized-radix KoggeStone embedded into BrentKung
    int offset = 0;
    for (int j = e_size; j > 1; j >>= LANE_LOG)
    {
        for (int i = gtid.x; i < j; i += GROUP_SIZE)
        {
            const int t = ((i + 1) << offset) - 1;
            prefixSumBuffer[t] += WavePrefixSum(prefixSumBuffer[t]);
        }
        DeviceMemoryBarrierWithGroupSync();
        offset += LANE_LOG;
    }
    
    //Downsweep
    //Warp-sized radix propogation fans
    offset = LANE_LOG;
    for (j = 1 << LANE_LOG; j < e_size; j <<= LANE_LOG)
    {
        for (int i = gtid.x + j; i < e_size; i += GROUP_SIZE)
            if ((i & (j << LANE_LOG) - 1) >= j)         
                if ((i + 1 & j - 1) != 0)                
                    prefixSumBuffer[i] += prefixSumBuffer[((i >> offset) << offset) - 1];
        DeviceMemoryBarrierWithGroupSync();
        offset += LANE_LOG;
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void RadixBrentKungFused(int3 gtid : SV_GroupThreadID)
{
    int offset = 0;
    for (int j = 1; j < (e_size >> 1); j <<= LANE_LOG)
    {
        for (int i = gtid.x; i < (e_size >> offset); i += GROUP_SIZE)
            prefixSumBuffer[((i + 1) << offset) - 1] += WavePrefixSum(prefixSumBuffer[((i + 1) << offset) - 1]);
        DeviceMemoryBarrierWithGroupSync();
        
        for (int i = gtid.x + j; i < e_size; i += GROUP_SIZE)
            if ((i & (j << LANE_LOG) - 1) >= j)         
                if ((i + 1 & j - 1) != 0)                
                    prefixSumBuffer[i] += WaveReadLaneFirst(prefixSumBuffer[((i >> offset) << offset) - 1]);
        offset += LANE_LOG;
    }
    DeviceMemoryBarrierWithGroupSync();
    
    for (int i = gtid.x + j; i < e_size; i += GROUP_SIZE)              
        prefixSumBuffer[i] += WaveReadLaneFirst(prefixSumBuffer[((i >> offset) << offset) - 1]);
}

[numthreads(GROUP_SIZE, 1, 1)]
void RadixSklanskyLarge(int3 gtid : SV_GroupThreadID)
{
    //Warp-sized radix Kogge-Stone
    for (int i = gtid.x; i < e_size; i += GROUP_SIZE)
        prefixSumBuffer[i] += WavePrefixSum(prefixSumBuffer[i]);
    DeviceMemoryBarrierWithGroupSync();
    
    //Warp-sized radix Sklansky propogation fan
    int offset = LANE_LOG;
    for (int j = 1 << LANE_LOG; j < e_size; j <<= 1)
    {
        for (int i = gtid.x + j; i < e_size; i += GROUP_SIZE)
            if ((i & j) != 0)
                prefixSumBuffer[i] += prefixSumBuffer[((i >> offset) << offset) - 1];
        DeviceMemoryBarrierWithGroupSync();
        ++offset;
    }
}

[numthreads(GROUP_SIZE, 1, 1)]
void RadixSklanskyAdvanced(int3 gtid : SV_GroupThreadID)
{
    //Warp-sized radix Kogge-Stone
    for (int i = gtid.x; i < e_size; i += GROUP_SIZE)
        prefixSumBuffer[i] += WavePrefixSum(prefixSumBuffer[i]);
    DeviceMemoryBarrierWithGroupSync();
    
    int offset = LANE_LOG;
    for (int j = 1 << LANE_LOG; j < e_size; j <<= 1)
    {
        for (int i = gtid.x; i < (e_size >> 1); i += GROUP_SIZE)
        {
            const int t = ((((i >> offset) << 1) + 1) << offset) + (i & (1 << offset) - 1);
            prefixSumBuffer[t] += WaveReadLaneFirst(prefixSumBuffer[((t >> offset) << offset) - 1]);
        }
        DeviceMemoryBarrierWithGroupSync();
        ++offset;
    }
}