/******************************************************************************
 * Exclusive Vectorized Chained Scan With Decoupled Lookback
 *
 * Variant: Raking warp-sized radix reduce scan using partitions of size equal to 
 *          maximum shared memory.
 *                    
 * Notes:   **Preprocessor macros must be manually changed for AMD**
 * 
 * Author:  Thomas Smith 8/7/2023
 *
 * Based off of Research by:
 *          Duane Merrill, Nvidia Corporation
 *          Michael Garland, Nvidia Corporation
 *          https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back
 *
 * Copyright (c) 2011, Duane Merrill.  All rights reserved.
 * Copyright (c) 2011-2022, NVIDIA CORPORATION.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *     * Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * Neither the name of the NVIDIA CORPORATION nor the
 *       names of its contributors may be used to endorse or promote products
 *       derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 ******************************************************************************/
#pragma use_dxc
#pragma kernel Init
#pragma kernel ChainedDecoupledExclusive
#pragma kernel Validate

#define PARTITION_SIZE      8192    //The size of a partition tile
#define PART_VEC_SIZE       2048    //The size in uint4 of a partition tile
#define GROUP_SIZE          512     //The number of threads in a group

#define VECTOR_MASK         3       //Mask of uint4 size aka 4 - 1
#define VECTOR_LOG          2       //log2(4) 

#define FLAG_NOT_READY  0           //Flag indicating this partition tile's local reduction is not ready
#define FLAG_AGGREGATE  1           //Flag indicating this partition tile's local reduction is ready
#define FLAG_INCLUSIVE  2           //Flag indicating this partition tile has summed all preceding tiles and added to its sum.
#define FLAG_MASK       3           //Mask used to retrieve the flag

#define PARTITIONS          ((e_size >> 13) + ((e_size & 8191) ? 1 : 0))            //hack divroundup
#define WAVE_INDEX          (gtid.x / WaveGetLaneCount())
#define WAVE_PART_SIZE      (PART_VEC_SIZE / GROUP_SIZE * WaveGetLaneCount())
#define WAVE_PART_START     (WAVE_INDEX * WAVE_PART_SIZE)
#define PARTITION_START     (partitionIndex * PART_VEC_SIZE)
#define WAVES_PER_GROUP     (GROUP_SIZE / WaveGetLaneCount())
#define IS_FIRST_WAVE       (gtid.x < WaveGetLaneCount())
#define IS_NOT_FIRST_WAVE   (gtid.x >= WaveGetLaneCount())
#define SPINE_INDEX         (((gtid.x + 1) * WAVE_PART_SIZE) - 1)

extern int e_size;

globallycoherent RWBuffer<uint> b_state;
globallycoherent RWBuffer<uint> b_index;

RWStructuredBuffer<uint> b_prefixLoad;
RWBuffer<uint4> b_prefixSum;
RWBuffer<uint> b_timing;

groupshared uint4 g_sharedMem[PART_VEC_SIZE];

[numthreads(GROUP_SIZE, 1, 1)]
void Init(int3 id : SV_DispatchThreadID)
{
    if (id.x == 0)
        b_index[id.x] = 0;
    
    for (int i = id.x; i < e_size; i += GROUP_SIZE * 256)
        b_prefixLoad[i] = 1;
    
    for (int i = id.x; i < PARTITIONS; i += GROUP_SIZE * 256)
        b_state[i] = 0;
}

[numthreads(GROUP_SIZE, 1, 1)]
void ChainedDecoupledExclusive(int3 gtid : SV_GroupThreadID, int3 gid : SV_GroupID)
{
    //Acquire the partition index
    int partitionIndex;
    if (WAVE_INDEX == 0 && WaveGetLaneIndex() == 0)
        InterlockedAdd(b_index[0], 1, g_sharedMem[0].x);
    GroupMemoryBarrierWithGroupSync();
    partitionIndex = WaveReadLaneAt(g_sharedMem[0].x, 0);
    GroupMemoryBarrierWithGroupSync();
    
    const int partSize = partitionIndex < PARTITIONS - 1 ? (e_size >> VECTOR_LOG) + (e_size & VECTOR_MASK ? 1 : 0) - PARTITION_START : PART_VEC_SIZE;
    int i = WaveGetLaneIndex() + WAVE_PART_START;
    if (i < partSize)
    {
        g_sharedMem[i] = b_prefixSum[i + PARTITION_START];
        
        uint t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].y;
        g_sharedMem[i].y = t;
        
        t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].z;
        g_sharedMem[i].z = t;
        
        t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].w;
        g_sharedMem[i].w = t;
        g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].x);
    }
            
    i += WaveGetLaneCount();
    if (i < partSize)
    {
        g_sharedMem[i] = b_prefixSum[i + PARTITION_START];
        
        uint t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].y;
        g_sharedMem[i].y = t;
        
        t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].z;
        g_sharedMem[i].z = t;
        
        t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].w;
        g_sharedMem[i].w = t;
        g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].x) + WaveReadLaneAt(g_sharedMem[i - 1].x, 0);
    }
            
    i += WaveGetLaneCount();
    if (i < partSize)
    {
        g_sharedMem[i] = b_prefixSum[i + PARTITION_START];
        
        uint t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].y;
        g_sharedMem[i].y = t;
        
        t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].z;
        g_sharedMem[i].z = t;
        
        t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].w;
        g_sharedMem[i].w = t;
        g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].x) + WaveReadLaneAt(g_sharedMem[i - 1].x, 0);
    }
            
    i += WaveGetLaneCount();
    if (i < partSize)
    {
        g_sharedMem[i] = b_prefixSum[i + PARTITION_START];
        
        uint t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].y;
        g_sharedMem[i].y = t;
        
        t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].z;
        g_sharedMem[i].z = t;
        
        t = g_sharedMem[i].x;
        g_sharedMem[i].x += g_sharedMem[i].w;
        g_sharedMem[i].w = t;
        g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].x) + WaveReadLaneAt(g_sharedMem[i - 1].x, 0);
    }
    GroupMemoryBarrierWithGroupSync();
        
    if (WAVE_INDEX == 0 && WaveGetLaneIndex() < WAVES_PER_GROUP)
        g_sharedMem[SPINE_INDEX] += WavePrefixSum(g_sharedMem[SPINE_INDEX].x);
    GroupMemoryBarrierWithGroupSync();
            
    //Set flag payload
    if (WAVE_INDEX == 0 && WaveGetLaneIndex() == 0)
    {
        if (partitionIndex == 0)
            InterlockedOr(b_state[partitionIndex], FLAG_INCLUSIVE ^ (g_sharedMem[PART_VEC_SIZE - 1].x << 2));
        else
            InterlockedOr(b_state[partitionIndex], FLAG_AGGREGATE ^ (g_sharedMem[PART_VEC_SIZE - 1].x << 2));
    }
    
    //Lookback
    uint aggregate = 0;
    if (partitionIndex)
    {
        if (WAVE_INDEX == 0)
        {
            for (int k = partitionIndex - WaveGetLaneIndex() - 1; 0 <= k;)
            {
                uint flagPayload = b_state[k];
                const int inclusiveIndex = WaveActiveMin(WaveGetLaneIndex() + WaveGetLaneCount() - ((flagPayload & FLAG_MASK) == FLAG_INCLUSIVE ? WaveGetLaneCount() : 0));
                const int gapIndex = WaveActiveMin(WaveGetLaneIndex() + WaveGetLaneCount() - ((flagPayload & FLAG_MASK) == FLAG_NOT_READY ? WaveGetLaneCount() : 0));
                if (inclusiveIndex < gapIndex)
                {
                    aggregate += WaveActiveSum(WaveGetLaneIndex() <= inclusiveIndex ? (flagPayload >> 2) : 0);
                    if (WaveGetLaneIndex() == 0)
                    {
                        InterlockedAdd(b_state[partitionIndex], 1 | aggregate << 2);
                        g_sharedMem[PART_VEC_SIZE - 1].x = aggregate;
                    }
                    break;
                }
                else
                {
                    if (gapIndex < WaveGetLaneCount())
                    {
                        aggregate += WaveActiveSum(WaveGetLaneIndex() < gapIndex ? (flagPayload >> 2) : 0);
                        k -= gapIndex;
                    }
                    else
                    {
                        aggregate += WaveActiveSum(flagPayload >> 2);
                        k -= WaveGetLaneCount();
                    }
                }
            }
        }
        GroupMemoryBarrierWithGroupSync();
            
        //propogate aggregate values
        if (WAVE_INDEX || WaveGetLaneIndex())
            aggregate = WaveReadLaneAt(g_sharedMem[PART_VEC_SIZE - 1].x, 1);
    }
            
    const uint prev = (WAVE_INDEX ? WaveReadLaneAt(g_sharedMem[WaveGetLaneIndex() + WAVE_PART_START - 1].x, 0) : 0) + aggregate;
    GroupMemoryBarrierWithGroupSync();
            
    if (i < partSize)
    {
        g_sharedMem[i].x = g_sharedMem[i - 1].x + (WaveGetLaneIndex() + 1 != WaveGetLaneCount() ? 0 : prev - aggregate);
        b_prefixSum[i + PARTITION_START] = g_sharedMem[i] + (WaveGetLaneIndex() + 1 != WaveGetLaneCount() ? prev : aggregate);
    }
    
    i -= WaveGetLaneCount();
    if (i < partSize)
    {
        g_sharedMem[i].x = g_sharedMem[i - 1].x;
        b_prefixSum[i + PARTITION_START] = g_sharedMem[i] + prev;
    }
            
    i -= WaveGetLaneCount();
    if (i < partSize)
    {
        g_sharedMem[i].x = g_sharedMem[i - 1].x;
        b_prefixSum[i + PARTITION_START] = g_sharedMem[i] + prev;
    }
            
    i -= WaveGetLaneCount();
    if (i < partSize)
    {
        g_sharedMem[i].x = WaveGetLaneIndex() ? g_sharedMem[i - 1].x : 0;
        b_prefixSum[i + PARTITION_START] = g_sharedMem[i] + prev;
    }
    
    //for timing the kernel
    if (WAVE_INDEX == 0 && WaveGetLaneIndex() == 0)
        b_timing[gid.x] = 0;
}

//---------------------------VALIDATION UTILITY---------------------------
//Perform validation for non-random input on GPU to massively increase speed
globallycoherent RWStructuredBuffer<uint> b_validate;
AppendStructuredBuffer<uint3> b_error;
#define VAL_THREADS     512

[numthreads(VAL_THREADS, 1, 1)]
void Validate(int3 gtid : SV_GroupThreadID, int3 gid : SV_GroupID)
{
    if (gid.x < PARTITIONS - 1)
    {
        for (int i = gtid.x; i < PART_VEC_SIZE; i += VAL_THREADS)
        {
            const uint t = i + gid.x * PART_VEC_SIZE;
            const uint4 sums = b_prefixSum[t];
        
            for (int k = 0; k < 4; ++k)
            {
                if (sums[k] != t * 4 + k)
                {
                    uint errCount;
                    InterlockedAdd(b_validate[0], 1, errCount);
                    if (errCount < 1024)
                        b_error.Append(uint3(t * 4 + k, sums[k], t * 4 + k));
                }
            }
        }
    }
    else
    {
        const int partSize = (e_size >> VECTOR_LOG) + (e_size & VECTOR_MASK ? 1 : 0) - gid.x * PART_VEC_SIZE;
        for (int i = gtid.x; i < partSize; i += VAL_THREADS)
        {
            const uint t = i + gid.x * PART_VEC_SIZE;
            const uint4 sums = b_prefixSum[t];
        
            for (int k = 0; k < 4; ++k)
            {
                const uint t2 = t * 4 + k;
                if (t2 < e_size)
                {
                    if (sums[k] != t2)
                    {
                        uint errCount;
                        InterlockedAdd(b_validate[0], 1, errCount);
                        if (errCount < 1024)
                            b_error.Append(uint3(t2, sums[k], t2));
                    }
                }
                
            }
        }
    }
}