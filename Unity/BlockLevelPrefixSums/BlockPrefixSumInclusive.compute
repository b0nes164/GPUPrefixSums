/*****************************************************************************
 * Block Level Inclusive Vectorized Warp-Scan Raking Reduce then Scan
 * 
 * Variant: Partition tile equal to the maximum size of shared memory.
 *          The partition is equally subdivided amongst the warps.
 *          Each thread performs a serial scan on a single vector input, followed
 *          by warp wide KoggeStone Scans on the w element of each thread vector. This 
 *          proceeds until the entire warp partition is consumed. Finally, the block wide 
 *          aggregate is computed using a single Kogge-Stone warp-scan along the spine
 *          of partition, and the result is propagated directly back into device memory
 * 
 * Author:  Thomas Smith 8/5/2023
 * 
 * License: The Unlicense
 *          This is free and unencumbered software released into the public domain.
 *          For more information, please refer to the repository license or <https://unlicense.org>
 *
 ******************************************************************************/
#pragma use_dxc
#pragma kernel Init
#pragma kernel BlockPrefixSumInclusive
#pragma kernel Validate

#define PARTITION_SIZE      8192    //Size of threadblock partition
#define PART_VEC_SIZE       2048    //Size of the threadBlock partition in uint4
#define GROUP_SIZE          1024    //The number of threads in the threadBlock

#define VECTOR_MASK         3       //the mask of the size of the VECTOR for faster div round up
#define VECTOR_LOG          2       //log2(VECTOR_SIZE), log2(4)

#define WAVE_INDEX          (gtid.x / WaveGetLaneCount())
#define SPINE_INDEX         (((gtid.x + 1) * wavePartitionSize) - 1)
#define PARTITIONS          ((e_size >> 13) + ((e_size & 8191) ? 1 : 0))            //hack divroundup
#define VEC_PART_START      (partitionIndex * PART_VEC_SIZE)
#define WAVE_PART_START     (WAVE_INDEX * wavePartitionSize)
#define WAVE_PART_END       ((WAVE_INDEX + 1) * wavePartitionSize)
#define WAVES_PER_GROUP     (GROUP_SIZE / WaveGetLaneCount())
#define IS_FIRST_WAVE       (gtid.x < WaveGetLaneCount())
#define IS_NOT_FIRST_WAVE   (gtid.x >= WaveGetLaneCount())

extern int e_size;

RWStructuredBuffer<uint> b_prefixLoad;
RWBuffer<uint4> b_prefixSum;
RWBuffer<uint> b_timing;
groupshared uint4 g_sharedMem[PART_VEC_SIZE];

[numthreads(GROUP_SIZE, 1, 1)]
void Init(int3 id : SV_DispatchThreadID)
{
    for (int i = id.x; i < e_size; i += GROUP_SIZE * 256)
        b_prefixLoad[i] = 1;
    
    if (id.x == 0)
        b_timing[id.x] = 0;
}

[numthreads(GROUP_SIZE, 1, 1)]
void BlockPrefixSumInclusive(int3 gtid : SV_GroupThreadID)
{
    uint aggregate = 0;
    const uint wavePartitionSize = PART_VEC_SIZE / GROUP_SIZE * WaveGetLaneCount();
    for (int partitionIndex = 0; partitionIndex < PARTITIONS - 1; ++partitionIndex)
    {
        int i = WaveGetLaneIndex() + WAVE_PART_START;
        g_sharedMem[i] = b_prefixSum[i + VEC_PART_START];
        g_sharedMem[i].y += g_sharedMem[i].x;
        g_sharedMem[i].z += g_sharedMem[i].y;
        g_sharedMem[i].w += g_sharedMem[i].z;
        g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w);
        
        i += WaveGetLaneCount();
        g_sharedMem[i] = b_prefixSum[i + VEC_PART_START];
        g_sharedMem[i].y += g_sharedMem[i].x;
        g_sharedMem[i].z += g_sharedMem[i].y;
        g_sharedMem[i].w += g_sharedMem[i].z;
        g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w) + WaveReadLaneFirst(g_sharedMem[i - 1].w);
        GroupMemoryBarrierWithGroupSync();
        
        if (IS_FIRST_WAVE && WaveGetLaneIndex() < WAVES_PER_GROUP)
            g_sharedMem[SPINE_INDEX] += WavePrefixSum(g_sharedMem[SPINE_INDEX].w) + aggregate;
        GroupMemoryBarrierWithGroupSync();

        const uint prev = IS_NOT_FIRST_WAVE ? WaveReadLaneFirst(g_sharedMem[WaveGetLaneIndex() + WAVE_PART_START - 1].w) : aggregate;
        b_prefixSum[i + VEC_PART_START] = g_sharedMem[i] + (WaveGetLaneIndex() + 1 != WaveGetLaneCount() ? prev : 0);
        
        i -= WaveGetLaneCount();
        b_prefixSum[i + VEC_PART_START] = g_sharedMem[i] + prev;
        
        aggregate = WaveReadLaneFirst(g_sharedMem[PART_VEC_SIZE - 1].w);
        GroupMemoryBarrierWithGroupSync();
    }

    const int finalPartSize = (e_size >> VECTOR_LOG) + (e_size & VECTOR_MASK ? 1 : 0) - VEC_PART_START;
    
    int i = WaveGetLaneIndex() + WAVE_PART_START;
    if (i < finalPartSize)
    {
        g_sharedMem[i] = b_prefixSum[i + VEC_PART_START];
        g_sharedMem[i].y += g_sharedMem[i].x;
        g_sharedMem[i].z += g_sharedMem[i].y;
        g_sharedMem[i].w += g_sharedMem[i].z;
        g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w);
    }
    
    i += WaveGetLaneCount();
    if (i < finalPartSize)
    {
        g_sharedMem[i] = b_prefixSum[i + VEC_PART_START];
        g_sharedMem[i].y += g_sharedMem[i].x;
        g_sharedMem[i].z += g_sharedMem[i].y;
        g_sharedMem[i].w += g_sharedMem[i].z;
        g_sharedMem[i] += WavePrefixSum(g_sharedMem[i].w) + WaveReadLaneFirst(g_sharedMem[i - 1].w);
    }
    GroupMemoryBarrierWithGroupSync();
    
    if (IS_FIRST_WAVE && WaveGetLaneIndex() < WAVES_PER_GROUP)
        g_sharedMem[SPINE_INDEX] += WavePrefixSum(g_sharedMem[SPINE_INDEX].w) + aggregate;
    GroupMemoryBarrierWithGroupSync();
    
    const uint prev = IS_NOT_FIRST_WAVE ? WaveReadLaneFirst(g_sharedMem[WaveGetLaneIndex() + WAVE_PART_START - 1].w) : aggregate;
    if (i < finalPartSize)
        b_prefixSum[i + VEC_PART_START] = g_sharedMem[i] + (WaveGetLaneIndex() + 1 != WaveGetLaneCount() ? prev : 0);
    
    i -= WaveGetLaneCount();
    if (i < finalPartSize)
        b_prefixSum[i + VEC_PART_START] = g_sharedMem[i] + prev;
    GroupMemoryBarrierWithGroupSync();
    
    //for timing the kernel
    if (WaveGetLaneIndex() == 0 && WAVE_INDEX == 0)
        b_timing[0] = 1;
}

//---------------------------VALIDATION UTILITY---------------------------
//Perform validation for non-random input on GPU to massively increase speed
globallycoherent RWStructuredBuffer<uint> b_validate;
AppendStructuredBuffer<uint3> b_error;
#define VAL_THREADS     512

[numthreads(VAL_THREADS, 1, 1)]
void Validate(int3 gtid : SV_GroupThreadID, int3 gid : SV_GroupID)
{
    if (gid.x < PARTITIONS - 1)
    {
        for (int i = gtid.x; i < PART_VEC_SIZE; i += VAL_THREADS)
        {
            const uint t = i + gid.x * PART_VEC_SIZE;
            const uint4 sums = b_prefixSum[t];
        
            for (int k = 0; k < 4; ++k)
            {
                if (sums[k] != t * 4 + k + 1)
                {
                    uint errCount;
                    InterlockedAdd(b_validate[0], 1, errCount);
                    if (errCount < 1024)
                        b_error.Append(uint3(t * 4 + k, sums[k], t * 4 + k + 1));
                }
            }
        }
    }
    else
    {
        const int partSize = (e_size >> VECTOR_LOG) + (e_size & VECTOR_MASK ? 1 : 0) - gid.x * PART_VEC_SIZE;
        for (int i = gtid.x; i < partSize; i += VAL_THREADS)
        {
            const uint t = i + gid.x * PART_VEC_SIZE;
            const uint4 sums = b_prefixSum[t];
        
            for (int k = 0; k < 4; ++k)
            {
                const uint t2 = t * 4 + k;
                if (t2 < e_size)
                {
                    if (sums[k] != t2 + 1)
                    {
                        uint errCount;
                        InterlockedAdd(b_validate[0], 1, errCount);
                        if (errCount < 1024)
                            b_error.Append(uint3(t2, sums[k], t2 + 1));
                    }
                }
                
            }
        }
    }
}